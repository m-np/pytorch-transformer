{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c77d07",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006994,
     "end_time": "2024-01-18T04:02:54.999484",
     "exception": false,
     "start_time": "2024-01-18T04:02:54.992490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Notebook\n",
    "\n",
    "Library Imports for the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232cbc87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:02:55.014099Z",
     "iopub.status.busy": "2024-01-18T04:02:55.013666Z",
     "iopub.status.idle": "2024-01-18T04:02:55.026720Z",
     "shell.execute_reply": "2024-01-18T04:02:55.025771Z"
    },
    "papermill": {
     "duration": 0.023238,
     "end_time": "2024-01-18T04:02:55.029091",
     "exception": false,
     "start_time": "2024-01-18T04:02:55.005853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   # miscellaneous os interfaces\n",
    "import sys  # configuring python runtime environment\n",
    "import time # library for time manipulation, and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f14274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:02:55.043812Z",
     "iopub.status.busy": "2024-01-18T04:02:55.043080Z",
     "iopub.status.idle": "2024-01-18T04:02:55.047155Z",
     "shell.execute_reply": "2024-01-18T04:02:55.046367Z"
    },
    "papermill": {
     "duration": 0.014007,
     "end_time": "2024-01-18T04:02:55.049262",
     "exception": false,
     "start_time": "2024-01-18T04:02:55.035255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `datetime` to control and preceive the environment\n",
    "# in addition `pandas` also provides date time functionalities\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46953a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:02:55.064128Z",
     "iopub.status.busy": "2024-01-18T04:02:55.063381Z",
     "iopub.status.idle": "2024-01-18T04:02:55.083470Z",
     "shell.execute_reply": "2024-01-18T04:02:55.082190Z"
    },
    "papermill": {
     "duration": 0.030896,
     "end_time": "2024-01-18T04:02:55.086563",
     "exception": false,
     "start_time": "2024-01-18T04:02:55.055667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy      # dataframe is mutable\n",
    "from tqdm import tqdm     # progress bar for loops\n",
    "from uuid import uuid4 as UUID # unique identifier for objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f747559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:02:55.100859Z",
     "iopub.status.busy": "2024-01-18T04:02:55.100477Z",
     "iopub.status.idle": "2024-01-18T04:02:55.105841Z",
     "shell.execute_reply": "2024-01-18T04:02:55.104594Z"
    },
    "papermill": {
     "duration": 0.015591,
     "end_time": "2024-01-18T04:02:55.108464",
     "exception": false,
     "start_time": "2024-01-18T04:02:55.092873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8268b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:02:55.122643Z",
     "iopub.status.busy": "2024-01-18T04:02:55.122262Z",
     "iopub.status.idle": "2024-01-18T04:03:11.562729Z",
     "shell.execute_reply": "2024-01-18T04:03:11.561032Z"
    },
    "papermill": {
     "duration": 16.452313,
     "end_time": "2024-01-18T04:03:11.567077",
     "exception": false,
     "start_time": "2024-01-18T04:02:55.114764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be9a34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:11.583005Z",
     "iopub.status.busy": "2024-01-18T04:03:11.582597Z",
     "iopub.status.idle": "2024-01-18T04:03:16.226077Z",
     "shell.execute_reply": "2024-01-18T04:03:16.224250Z"
    },
    "papermill": {
     "duration": 4.655224,
     "end_time": "2024-01-18T04:03:16.229200",
     "exception": false,
     "start_time": "2024-01-18T04:03:11.573976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177dffe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.245070Z",
     "iopub.status.busy": "2024-01-18T04:03:16.244469Z",
     "iopub.status.idle": "2024-01-18T04:03:16.249673Z",
     "shell.execute_reply": "2024-01-18T04:03:16.248482Z"
    },
    "papermill": {
     "duration": 0.016085,
     "end_time": "2024-01-18T04:03:16.252093",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.236008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76dee3",
   "metadata": {
    "papermill": {
     "duration": 0.006216,
     "end_time": "2024-01-18T04:03:16.264872",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.258656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building model\n",
    "\n",
    "Create a transformer model from the original [transformer paper](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Lets start the build by understanding the fundamental block of transformers and build the entire model from here\n",
    "\n",
    "### Understanding Multi Head Attention (MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7396cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.280554Z",
     "iopub.status.busy": "2024-01-18T04:03:16.279857Z",
     "iopub.status.idle": "2024-01-18T04:03:16.295477Z",
     "shell.execute_reply": "2024-01-18T04:03:16.294313Z"
    },
    "papermill": {
     "duration": 0.026902,
     "end_time": "2024-01-18T04:03:16.298555",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.271653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnoptimizedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://jalammar.github.io/illustrated-transformer/\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.ModuleList([nn.Linear(self.dmodel, self.dk) for _ in range(self.h)]) # shape -> (dmodel, dk)\n",
    "        self.WK = nn.ModuleList([nn.Linear(self.dmodel, self.dk) for _ in range(self.h)]) # shape -> (dmodel, dk)\n",
    "        self.WV = nn.ModuleList([nn.Linear(self.dmodel, self.dv) for _ in range(self.h)]) # shape -> (dmodel, dv)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        \n",
    "#         self.attention_dropout = nn.Dropout(p=dropout_probability)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def attention(self, query, key, val):\n",
    "        \"\"\"\n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, dmodel, S/T)\n",
    "        key shape = (B, dmodel, S/T)\n",
    "        val shape = (B, dmodel, S/T)\n",
    "        \"\"\"\n",
    "        head = []\n",
    "        # Create multiple heads using SDP\n",
    "        for i in range(self.h):\n",
    "            Q = self.WQ[i](query) # shape -> (B, 1, dk)\n",
    "            K = self.WK[i](key)   # shape -> (B, 1, dk)\n",
    "            V = self.WV[i](val)   # shape -> (B, 1, dv)\n",
    "            score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dk) # shape -> (B, 1, 1)\n",
    "            score = self.softmax(score)\n",
    "            H = torch.matmul(score, V) # V Transpose not needed here as per the paper shape -> (B, 1, dk)\n",
    "            head.append(H)\n",
    "        return head\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \"\"\"\n",
    "        query = key = val = x # For visualization we use the same input for all shape = (B, 1, dmodel)\n",
    "        # Calculate multi head attentions for Q, K, V\n",
    "        head = self.attention(query, key, val)\n",
    "        # Concatenate multiple head along dim 1 as head shape = [B x 1 x dk]xh\n",
    "        # therefore resultant would be out shape = B x 1 x dk*h\n",
    "        out = torch.cat(head, axis=-1)\n",
    "        # Final token_representation shape = (B, (dmodel*h), dmodel)\n",
    "        token_representation = self.WO(out)  # shape = B x 1 x (dk*h=dmodel)\n",
    "        return token_representation\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6cc03b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.314239Z",
     "iopub.status.busy": "2024-01-18T04:03:16.313537Z",
     "iopub.status.idle": "2024-01-18T04:03:16.318573Z",
     "shell.execute_reply": "2024-01-18T04:03:16.317564Z"
    },
    "papermill": {
     "duration": 0.015621,
     "end_time": "2024-01-18T04:03:16.320812",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.305191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dk = dv = 64\n",
    "h = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a399db62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.336059Z",
     "iopub.status.busy": "2024-01-18T04:03:16.335631Z",
     "iopub.status.idle": "2024-01-18T04:03:16.499153Z",
     "shell.execute_reply": "2024-01-18T04:03:16.498221Z"
    },
    "papermill": {
     "duration": 0.174754,
     "end_time": "2024-01-18T04:03:16.502207",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.327453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnoptimizedMultiHeadAttention(\n",
      "  (WQ): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WK): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WV): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]          32,832\n",
      "            Linear-2                [-1, 1, 64]          32,832\n",
      "            Linear-3                [-1, 1, 64]          32,832\n",
      "           Softmax-4                 [-1, 1, 1]               0\n",
      "            Linear-5                [-1, 1, 64]          32,832\n",
      "            Linear-6                [-1, 1, 64]          32,832\n",
      "            Linear-7                [-1, 1, 64]          32,832\n",
      "           Softmax-8                 [-1, 1, 1]               0\n",
      "            Linear-9                [-1, 1, 64]          32,832\n",
      "           Linear-10                [-1, 1, 64]          32,832\n",
      "           Linear-11                [-1, 1, 64]          32,832\n",
      "          Softmax-12                 [-1, 1, 1]               0\n",
      "           Linear-13                [-1, 1, 64]          32,832\n",
      "           Linear-14                [-1, 1, 64]          32,832\n",
      "           Linear-15                [-1, 1, 64]          32,832\n",
      "          Softmax-16                 [-1, 1, 1]               0\n",
      "           Linear-17                [-1, 1, 64]          32,832\n",
      "           Linear-18                [-1, 1, 64]          32,832\n",
      "           Linear-19                [-1, 1, 64]          32,832\n",
      "          Softmax-20                 [-1, 1, 1]               0\n",
      "           Linear-21                [-1, 1, 64]          32,832\n",
      "           Linear-22                [-1, 1, 64]          32,832\n",
      "           Linear-23                [-1, 1, 64]          32,832\n",
      "          Softmax-24                 [-1, 1, 1]               0\n",
      "           Linear-25                [-1, 1, 64]          32,832\n",
      "           Linear-26                [-1, 1, 64]          32,832\n",
      "           Linear-27                [-1, 1, 64]          32,832\n",
      "          Softmax-28                 [-1, 1, 1]               0\n",
      "           Linear-29                [-1, 1, 64]          32,832\n",
      "           Linear-30                [-1, 1, 64]          32,832\n",
      "           Linear-31                [-1, 1, 64]          32,832\n",
      "          Softmax-32                 [-1, 1, 1]               0\n",
      "           Linear-33               [-1, 1, 512]         262,656\n",
      "================================================================\n",
      "Total params: 1,050,624\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 4.01\n",
      "Estimated Total Size (MB): 4.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = UnoptimizedMultiHeadAttention(dk, dv, h)\n",
    "print(net)\n",
    "summary(net, (1, 512)) # Input should be 1, dk*h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a940d1f",
   "metadata": {
    "papermill": {
     "duration": 0.006399,
     "end_time": "2024-01-18T04:03:16.515913",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.509514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now optimizing the multi head attention by removing the for loop and introducing matrix calculation for the optimization.\n",
    "\n",
    "We also refer to this [blog](https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a) to understand the MHA further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1f2f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.531266Z",
     "iopub.status.busy": "2024-01-18T04:03:16.530793Z",
     "iopub.status.idle": "2024-01-18T04:03:16.547437Z",
     "shell.execute_reply": "2024-01-18T04:03:16.546157Z"
    },
    "papermill": {
     "duration": 0.027434,
     "end_time": "2024-01-18T04:03:16.549962",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.522528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a\n",
    "    \n",
    "    Here we are clubbing all the linear layers together and duplicating the inputs and \n",
    "    then performing matrix multiplications\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WK = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WV = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \n",
    "        X has a size of (batch_size, seq_length, d_model)\n",
    "        Wq, Wk, and Wv have a size of (d_model, d_model)\n",
    "        \n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, S, dmodel)\n",
    "        key shape = (B, S, dmodel)\n",
    "        val shape = (B, S, dmodel)\n",
    "        \"\"\"\n",
    "        query = key = val = x # For visualization we use the same input for all shape = (B, S, dmodel)\n",
    "        \n",
    "        # Weight the queries\n",
    "        Q = self.WQ(query)     # shape -> (B, S, dmodel)\n",
    "        K = self.WK(key)       # shape -> (B, S, dmodel)\n",
    "        V = self.WV(val)       # shape -> (B, S, dmodel)\n",
    "        \n",
    "        # Separate last dimension to number of head and dk\n",
    "        batch_size = Q.size(0)   \n",
    "        Q = Q.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        K = K.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        V = V.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        \n",
    "        # each sequence is split across n_heads, with each head receiving seq_length tokens \n",
    "        # with d_key elements in each token instead of d_model.\n",
    "        Q = Q.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        K = K.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        V = V.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        \n",
    "        # dot product of Q and K\n",
    "        scaled_dot_product = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.dk)\n",
    "        scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        \n",
    "        # Create head \n",
    "        head = torch.matmul(scaled_dot_product, V)  # shape -> (B, h, S, S) * (B, h, S, dk) = (B, h, S, dk)\n",
    "        # Prepare the head to pass it through output linear layer\n",
    "        head = head.permute(0, 2, 1, 3).contiguous()  # shape -> (B, S, h, dk)\n",
    "        # Concatenate the head together\n",
    "        head = head.view(batch_size, -1, self.h* self.dk)  # shape -> (B, S, (h*dk = dmodel))\n",
    "        # Pass through output layer\n",
    "        token_representation = self.WO(head)\n",
    "        return token_representation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbdb57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T04:03:16.565446Z",
     "iopub.status.busy": "2024-01-18T04:03:16.565018Z",
     "iopub.status.idle": "2024-01-18T04:03:16.596263Z",
     "shell.execute_reply": "2024-01-18T04:03:16.594923Z"
    },
    "papermill": {
     "duration": 0.042184,
     "end_time": "2024-01-18T04:03:16.598993",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.556809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 10, 512]         262,656\n",
      "            Linear-2              [-1, 10, 512]         262,656\n",
      "            Linear-3              [-1, 10, 512]         262,656\n",
      "           Softmax-4            [-1, 8, 10, 10]               0\n",
      "            Linear-5              [-1, 10, 512]         262,656\n",
      "================================================================\n",
      "Total params: 1,050,624\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 4.01\n",
      "Estimated Total Size (MB): 4.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dk = dv = 64\n",
    "h = 8\n",
    "net = MultiHeadAttention(dk, dv, h)\n",
    "print(net)\n",
    "summary(net, (10, 512)) # Input should be S, (dk*h=dmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d9113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:50:53.448453Z",
     "iopub.status.busy": "2024-01-18T03:50:53.447059Z",
     "iopub.status.idle": "2024-01-18T03:50:53.458960Z",
     "shell.execute_reply": "2024-01-18T03:50:53.456634Z",
     "shell.execute_reply.started": "2024-01-18T03:50:53.448388Z"
    },
    "papermill": {
     "duration": 0.007189,
     "end_time": "2024-01-18T04:03:16.613436",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.606247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding Positonal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229b13a",
   "metadata": {
    "papermill": {
     "duration": 0.00645,
     "end_time": "2024-01-18T04:03:16.626734",
     "exception": false,
     "start_time": "2024-01-18T04:03:16.620284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2802718,
     "sourceId": 4853240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.489257,
   "end_time": "2024-01-18T04:03:17.557291",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-18T04:02:50.068034",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
