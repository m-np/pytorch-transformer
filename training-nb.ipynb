{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598297c2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.012415,
     "end_time": "2024-01-20T05:10:33.240316",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.227901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Notebook\n",
    "\n",
    "Library Imports for the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b00f6cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:33.266239Z",
     "iopub.status.busy": "2024-01-20T05:10:33.265176Z",
     "iopub.status.idle": "2024-01-20T05:10:33.278380Z",
     "shell.execute_reply": "2024-01-20T05:10:33.277378Z"
    },
    "papermill": {
     "duration": 0.028786,
     "end_time": "2024-01-20T05:10:33.280767",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.251981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   # miscellaneous os interfaces\n",
    "import sys  # configuring python runtime environment\n",
    "import time # library for time manipulation, and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79900c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:33.307188Z",
     "iopub.status.busy": "2024-01-20T05:10:33.306855Z",
     "iopub.status.idle": "2024-01-20T05:10:33.311063Z",
     "shell.execute_reply": "2024-01-20T05:10:33.310376Z"
    },
    "papermill": {
     "duration": 0.019813,
     "end_time": "2024-01-20T05:10:33.313026",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.293213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `datetime` to control and preceive the environment\n",
    "# in addition `pandas` also provides date time functionalities\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aac4a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:33.339671Z",
     "iopub.status.busy": "2024-01-20T05:10:33.338695Z",
     "iopub.status.idle": "2024-01-20T05:10:33.355150Z",
     "shell.execute_reply": "2024-01-20T05:10:33.354351Z"
    },
    "papermill": {
     "duration": 0.031922,
     "end_time": "2024-01-20T05:10:33.357316",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.325394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy      # dataframe is mutable\n",
    "from tqdm import tqdm     # progress bar for loops\n",
    "from uuid import uuid4 as UUID # unique identifier for objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3b332e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:33.383849Z",
     "iopub.status.busy": "2024-01-20T05:10:33.383548Z",
     "iopub.status.idle": "2024-01-20T05:10:33.388147Z",
     "shell.execute_reply": "2024-01-20T05:10:33.387353Z"
    },
    "papermill": {
     "duration": 0.020302,
     "end_time": "2024-01-20T05:10:33.390256",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.369954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3263ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:33.416902Z",
     "iopub.status.busy": "2024-01-20T05:10:33.416330Z",
     "iopub.status.idle": "2024-01-20T05:10:47.667292Z",
     "shell.execute_reply": "2024-01-20T05:10:47.666207Z"
    },
    "papermill": {
     "duration": 14.266658,
     "end_time": "2024-01-20T05:10:47.669628",
     "exception": false,
     "start_time": "2024-01-20T05:10:33.402970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75180bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:47.696134Z",
     "iopub.status.busy": "2024-01-20T05:10:47.695753Z",
     "iopub.status.idle": "2024-01-20T05:10:51.268493Z",
     "shell.execute_reply": "2024-01-20T05:10:51.267623Z"
    },
    "papermill": {
     "duration": 3.588711,
     "end_time": "2024-01-20T05:10:51.270995",
     "exception": false,
     "start_time": "2024-01-20T05:10:47.682284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085daaf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.297998Z",
     "iopub.status.busy": "2024-01-20T05:10:51.297485Z",
     "iopub.status.idle": "2024-01-20T05:10:51.303638Z",
     "shell.execute_reply": "2024-01-20T05:10:51.302596Z"
    },
    "papermill": {
     "duration": 0.021746,
     "end_time": "2024-01-20T05:10:51.305758",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.284012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d08ed5",
   "metadata": {
    "papermill": {
     "duration": 0.012208,
     "end_time": "2024-01-20T05:10:51.330516",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.318308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modify Pytorch summary for my own use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35017621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.355838Z",
     "iopub.status.busy": "2024-01-20T05:10:51.355232Z",
     "iopub.status.idle": "2024-01-20T05:10:51.364250Z",
     "shell.execute_reply": "2024-01-20T05:10:51.363589Z"
    },
    "papermill": {
     "duration": 0.023799,
     "end_time": "2024-01-20T05:10:51.366116",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.342317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "\n",
    "#     def register_hook(module):\n",
    "\n",
    "#         def hook(module, input, output):\n",
    "#             class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "#             module_idx = len(summary)\n",
    "\n",
    "#             m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "#             summary[m_key] = OrderedDict()\n",
    "#             print(11, module)\n",
    "#             print(111, input[0])\n",
    "#             summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "#             summary[m_key][\"input_shape\"][0] = batch_size\n",
    "#             if isinstance(output, (list, tuple)):\n",
    "#                 summary[m_key][\"output_shape\"] = [\n",
    "#                     [-1] + list(o.size())[1:] for o in output\n",
    "#                 ]\n",
    "#             else:\n",
    "#                 summary[m_key][\"output_shape\"] = list(output.size())\n",
    "#                 summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "#             params = 0\n",
    "#             if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "#                 params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "#                 summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "#             if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "#                 params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "#             summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "#         if (\n",
    "#             not isinstance(module, nn.Sequential)\n",
    "#             and not isinstance(module, nn.ModuleList)\n",
    "#             and not (module == model)\n",
    "#         ):\n",
    "#             hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "#     device = device.lower()\n",
    "#     assert device in [\n",
    "#         \"cuda\",\n",
    "#         \"cpu\",\n",
    "#     ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "#     if device == \"cuda\" and torch.cuda.is_available():\n",
    "#         dtype = torch.cuda.FloatTensor\n",
    "#     else:\n",
    "#         dtype = torch.FloatTensor\n",
    "\n",
    "#     # multiple inputs to the network\n",
    "#     if isinstance(input_size, tuple):\n",
    "#         input_size = [input_size]\n",
    "\n",
    "#     # batch_size of 2 for batchnorm\n",
    "#     x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n",
    "#     # print(type(x[0]))\n",
    "\n",
    "#     # create properties\n",
    "#     summary = OrderedDict()\n",
    "#     hooks = []\n",
    "\n",
    "#     # register hook\n",
    "#     model.apply(register_hook)\n",
    "\n",
    "#     # make a forward pass\n",
    "#     # print(x.shape)\n",
    "#     model(*x)\n",
    "\n",
    "#     # remove these hooks\n",
    "#     for h in hooks:\n",
    "#         h.remove()\n",
    "\n",
    "#     print(\"----------------------------------------------------------------\")\n",
    "#     line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "#     print(line_new)\n",
    "#     print(\"================================================================\")\n",
    "#     total_params = 0\n",
    "#     total_output = 0\n",
    "#     trainable_params = 0\n",
    "#     for layer in summary:\n",
    "#         # input_shape, output_shape, trainable, nb_params\n",
    "#         line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "#             layer,\n",
    "#             str(summary[layer][\"output_shape\"]),\n",
    "#             \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "#         )\n",
    "#         total_params += summary[layer][\"nb_params\"]\n",
    "#         total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "#         if \"trainable\" in summary[layer]:\n",
    "#             if summary[layer][\"trainable\"] == True:\n",
    "#                 trainable_params += summary[layer][\"nb_params\"]\n",
    "#         print(line_new)\n",
    "\n",
    "#     # assume 4 bytes/number (float on cuda).\n",
    "#     total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n",
    "#     total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
    "#     total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
    "#     total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "#     print(\"================================================================\")\n",
    "#     print(\"Total params: {0:,}\".format(total_params))\n",
    "#     print(\"Trainable params: {0:,}\".format(trainable_params))\n",
    "#     print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
    "#     print(\"----------------------------------------------------------------\")\n",
    "#     print(\"Input size (MB): %0.2f\" % total_input_size)\n",
    "#     print(\"Forward/backward pass size (MB): %0.2f\" % total_output_size)\n",
    "#     print(\"Params size (MB): %0.2f\" % total_params_size)\n",
    "#     print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n",
    "#     print(\"----------------------------------------------------------------\")\n",
    "#     # return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f457dd",
   "metadata": {
    "papermill": {
     "duration": 0.012489,
     "end_time": "2024-01-20T05:10:51.390414",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.377925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building model\n",
    "\n",
    "Create a transformer model from the original [transformer paper](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Lets start the build by understanding the fundamental block of transformers and build the entire model from here\n",
    "\n",
    "### Understanding Multi Head Attention (MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712290b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.415746Z",
     "iopub.status.busy": "2024-01-20T05:10:51.415036Z",
     "iopub.status.idle": "2024-01-20T05:10:51.429215Z",
     "shell.execute_reply": "2024-01-20T05:10:51.428576Z"
    },
    "papermill": {
     "duration": 0.028908,
     "end_time": "2024-01-20T05:10:51.431134",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.402226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnoptimizedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://jalammar.github.io/illustrated-transformer/\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.ModuleList([nn.Linear(self.dmodel, self.dk) for _ in range(self.h)]) # shape -> (dmodel, dk)\n",
    "        self.WK = nn.ModuleList([nn.Linear(self.dmodel, self.dk) for _ in range(self.h)]) # shape -> (dmodel, dk)\n",
    "        self.WV = nn.ModuleList([nn.Linear(self.dmodel, self.dv) for _ in range(self.h)]) # shape -> (dmodel, dv)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        \n",
    "#         self.attention_dropout = nn.Dropout(p=dropout_probability)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def attention(self, query, key, val):\n",
    "        \"\"\"\n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, dmodel, S/T)\n",
    "        key shape = (B, dmodel, S/T)\n",
    "        val shape = (B, dmodel, S/T)\n",
    "        \"\"\"\n",
    "        head = []\n",
    "        # Create multiple heads using SDP\n",
    "        for i in range(self.h):\n",
    "            Q = self.WQ[i](query) # shape -> (B, 1, dk)\n",
    "            K = self.WK[i](key)   # shape -> (B, 1, dk)\n",
    "            V = self.WV[i](val)   # shape -> (B, 1, dv)\n",
    "            score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dk) # shape -> (B, 1, 1)\n",
    "            score = self.softmax(score)\n",
    "            H = torch.matmul(score, V) # V Transpose not needed here as per the paper shape -> (B, 1, dk)\n",
    "            head.append(H)\n",
    "        return head\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \"\"\"\n",
    "        query = key = val = x # For visualization we use the same input for all shape = (B, 1, dmodel)\n",
    "        # Calculate multi head attentions for Q, K, V\n",
    "        head = self.attention(query, key, val)\n",
    "        # Concatenate multiple head along dim 1 as head shape = [B x 1 x dk]xh\n",
    "        # therefore resultant would be out shape = B x 1 x dk*h\n",
    "        out = torch.cat(head, axis=-1)\n",
    "        # Final token_representation shape = (B, (dmodel*h), dmodel)\n",
    "        token_representation = self.WO(out)  # shape = B x 1 x (dk*h=dmodel)\n",
    "        return token_representation\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e54bb0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.456678Z",
     "iopub.status.busy": "2024-01-20T05:10:51.456193Z",
     "iopub.status.idle": "2024-01-20T05:10:51.460273Z",
     "shell.execute_reply": "2024-01-20T05:10:51.459592Z"
    },
    "papermill": {
     "duration": 0.019043,
     "end_time": "2024-01-20T05:10:51.462153",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.443110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dk = dv = 64\n",
    "h = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af29744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.487398Z",
     "iopub.status.busy": "2024-01-20T05:10:51.486880Z",
     "iopub.status.idle": "2024-01-20T05:10:51.607644Z",
     "shell.execute_reply": "2024-01-20T05:10:51.606568Z"
    },
    "papermill": {
     "duration": 0.136186,
     "end_time": "2024-01-20T05:10:51.610056",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.473870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnoptimizedMultiHeadAttention(\n",
      "  (WQ): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WK): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WV): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]          32,832\n",
      "            Linear-2                [-1, 1, 64]          32,832\n",
      "            Linear-3                [-1, 1, 64]          32,832\n",
      "           Softmax-4                 [-1, 1, 1]               0\n",
      "            Linear-5                [-1, 1, 64]          32,832\n",
      "            Linear-6                [-1, 1, 64]          32,832\n",
      "            Linear-7                [-1, 1, 64]          32,832\n",
      "           Softmax-8                 [-1, 1, 1]               0\n",
      "            Linear-9                [-1, 1, 64]          32,832\n",
      "           Linear-10                [-1, 1, 64]          32,832\n",
      "           Linear-11                [-1, 1, 64]          32,832\n",
      "          Softmax-12                 [-1, 1, 1]               0\n",
      "           Linear-13                [-1, 1, 64]          32,832\n",
      "           Linear-14                [-1, 1, 64]          32,832\n",
      "           Linear-15                [-1, 1, 64]          32,832\n",
      "          Softmax-16                 [-1, 1, 1]               0\n",
      "           Linear-17                [-1, 1, 64]          32,832\n",
      "           Linear-18                [-1, 1, 64]          32,832\n",
      "           Linear-19                [-1, 1, 64]          32,832\n",
      "          Softmax-20                 [-1, 1, 1]               0\n",
      "           Linear-21                [-1, 1, 64]          32,832\n",
      "           Linear-22                [-1, 1, 64]          32,832\n",
      "           Linear-23                [-1, 1, 64]          32,832\n",
      "          Softmax-24                 [-1, 1, 1]               0\n",
      "           Linear-25                [-1, 1, 64]          32,832\n",
      "           Linear-26                [-1, 1, 64]          32,832\n",
      "           Linear-27                [-1, 1, 64]          32,832\n",
      "          Softmax-28                 [-1, 1, 1]               0\n",
      "           Linear-29                [-1, 1, 64]          32,832\n",
      "           Linear-30                [-1, 1, 64]          32,832\n",
      "           Linear-31                [-1, 1, 64]          32,832\n",
      "          Softmax-32                 [-1, 1, 1]               0\n",
      "           Linear-33               [-1, 1, 512]         262,656\n",
      "================================================================\n",
      "Total params: 1,050,624\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 4.01\n",
      "Estimated Total Size (MB): 4.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = UnoptimizedMultiHeadAttention(dk, dv, h)\n",
    "print(net)\n",
    "summary(net, (1, 512)) # Input should be 1, dk*h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bc207",
   "metadata": {
    "papermill": {
     "duration": 0.012133,
     "end_time": "2024-01-20T05:10:51.634911",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.622778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now optimizing the multi head attention by removing the for loop and introducing matrix calculation for the optimization.\n",
    "\n",
    "We also refer to this [blog](https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc7836d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.661514Z",
     "iopub.status.busy": "2024-01-20T05:10:51.661145Z",
     "iopub.status.idle": "2024-01-20T05:10:51.676206Z",
     "shell.execute_reply": "2024-01-20T05:10:51.675554Z"
    },
    "papermill": {
     "duration": 0.030369,
     "end_time": "2024-01-20T05:10:51.678238",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.647869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention1(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a\n",
    "    \n",
    "    Here we are clubbing all the linear layers together and duplicating the inputs and \n",
    "    then performing matrix multiplications\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WK = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WV = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \n",
    "        X has a size of (batch_size, seq_length, d_model)\n",
    "        Wq, Wk, and Wv have a size of (d_model, d_model)\n",
    "        \n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, S, dmodel)\n",
    "        key shape = (B, S, dmodel)\n",
    "        val shape = (B, S, dmodel)\n",
    "        \"\"\"\n",
    "#         TODO: Define the inputs properly\n",
    "        query = key = val = x # For visualization we use the same input for all shape = (B, S, dmodel)\n",
    "        \n",
    "        # Weight the queries\n",
    "        Q = self.WQ(query)     # shape -> (B, S, dmodel)\n",
    "        K = self.WK(key)       # shape -> (B, S, dmodel)\n",
    "        V = self.WV(val)       # shape -> (B, S, dmodel)\n",
    "        \n",
    "        # Separate last dimension to number of head and dk\n",
    "        batch_size = Q.size(0)   \n",
    "        Q = Q.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        K = K.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        V = V.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        \n",
    "        # each sequence is split across n_heads, with each head receiving seq_length tokens \n",
    "        # with d_key elements in each token instead of d_model.\n",
    "        Q = Q.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        K = K.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        V = V.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        \n",
    "        # dot product of Q and K\n",
    "        scaled_dot_product = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.dk)\n",
    "        scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        \n",
    "        # Create head \n",
    "        head = torch.matmul(scaled_dot_product, V)  # shape -> (B, h, S, S) * (B, h, S, dk) = (B, h, S, dk)\n",
    "        # Prepare the head to pass it through output linear layer\n",
    "        head = head.permute(0, 2, 1, 3).contiguous()  # shape -> (B, S, h, dk)\n",
    "        # Concatenate the head together\n",
    "        head = head.view(batch_size, -1, self.h* self.dk)  # shape -> (B, S, (h*dk = dmodel))\n",
    "        # Pass through output layer\n",
    "        token_representation = self.WO(head)\n",
    "        return token_representation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f528de40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.704360Z",
     "iopub.status.busy": "2024-01-20T05:10:51.704052Z",
     "iopub.status.idle": "2024-01-20T05:10:51.728721Z",
     "shell.execute_reply": "2024-01-20T05:10:51.727566Z"
    },
    "papermill": {
     "duration": 0.039841,
     "end_time": "2024-01-20T05:10:51.730727",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.690886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention1(\n",
      "  (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 10, 512]         262,656\n",
      "            Linear-2              [-1, 10, 512]         262,656\n",
      "            Linear-3              [-1, 10, 512]         262,656\n",
      "           Softmax-4            [-1, 8, 10, 10]               0\n",
      "            Linear-5              [-1, 10, 512]         262,656\n",
      "================================================================\n",
      "Total params: 1,050,624\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 4.01\n",
      "Estimated Total Size (MB): 4.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dk = dv = 64\n",
    "h = 8\n",
    "net = MultiHeadAttention1(dk, dv, h)\n",
    "print(net)\n",
    "summary(net, (10, 512)) # Input should be S, (dk*h=dmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58c772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:50:53.448453Z",
     "iopub.status.busy": "2024-01-18T03:50:53.447059Z",
     "iopub.status.idle": "2024-01-18T03:50:53.458960Z",
     "shell.execute_reply": "2024-01-18T03:50:53.456634Z",
     "shell.execute_reply.started": "2024-01-18T03:50:53.448388Z"
    },
    "papermill": {
     "duration": 0.011977,
     "end_time": "2024-01-20T05:10:51.755271",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.743294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding Positonal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc9dea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.781252Z",
     "iopub.status.busy": "2024-01-20T05:10:51.780697Z",
     "iopub.status.idle": "2024-01-20T05:10:51.788218Z",
     "shell.execute_reply": "2024-01-20T05:10:51.787563Z"
    },
    "papermill": {
     "duration": 0.02281,
     "end_time": "2024-01-20T05:10:51.790233",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.767423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding lookup table which is used by the positional embedding block.\n",
    "    Embedding lookup table is shared across input and output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, dmodel):\n",
    "        \"\"\"\n",
    "        Embedding lookup needs a vocab size and model dimension size matrix for \n",
    "        creating lookups\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_lookup = nn.Embedding(vocab_size, dmodel)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dmodel = dmodel\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        For a given token lookup the embedding vector\n",
    "        \n",
    "        As per the paper, we also multiply the embedding vector with sqrt of dmodel \n",
    "        \"\"\"\n",
    "        token_ids = torch.Tensor([[0, 0], [1, 1]]).type(torch.IntTensor)\n",
    "#         TODO: delete the above\n",
    "        # Since tokens -> shape -> (batch_size, token)\n",
    "        assert token_ids.ndim == 2, f'Expected: (batch size, max token sequence length), got {token_ids.shape}'\n",
    "        embedding_vector = self.embedding_lookup(token_ids)\n",
    "        \n",
    "        return embedding_vector * math.sqrt(self.dmodel)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150c427f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.815926Z",
     "iopub.status.busy": "2024-01-20T05:10:51.815427Z",
     "iopub.status.idle": "2024-01-20T05:10:51.829200Z",
     "shell.execute_reply": "2024-01-20T05:10:51.828522Z"
    },
    "papermill": {
     "duration": 0.02883,
     "end_time": "2024-01-20T05:10:51.831129",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.802299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(\n",
      "  (embedding_lookup): Embedding(100, 512)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1               [-1, 2, 512]          51,200\n",
      "================================================================\n",
      "Total params: 51,200\n",
      "Trainable params: 51,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "dmodel = dk*h\n",
    "net = Embedding(vocab_size, dmodel)\n",
    "print(net)\n",
    "summary(net, input_size=([[2]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805caa8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.858020Z",
     "iopub.status.busy": "2024-01-20T05:10:51.857417Z",
     "iopub.status.idle": "2024-01-20T05:10:51.868155Z",
     "shell.execute_reply": "2024-01-20T05:10:51.867507Z"
    },
    "papermill": {
     "duration": 0.02639,
     "end_time": "2024-01-20T05:10:51.870039",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.843649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dmodel, max_seq_length = 5000, pdropout = 0.1,):\n",
    "        \"\"\"\n",
    "        dmodel(int): model dimensions\n",
    "        max_seq_length(int): Maximum input sequence length\n",
    "        pdropout(float): Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "        # Calculate frequencies\n",
    "        position_ids = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        # -ve sign is added because the exponents are inverted when you multiply position and frequencies\n",
    "        frequencies = torch.pow(10000, -torch.arange(0, dmodel, 2, dtype = torch.float)/ dmodel) \n",
    "        \n",
    "        # Create positional encoding table\n",
    "        positional_encoding_table = torch.zeros(max_seq_length, dmodel)\n",
    "        # Fill the table with even entries with sin and odd entries with cosine\n",
    "        positional_encoding_table[:, 0::2] = torch.sin(position_ids * frequencies)\n",
    "        positional_encoding_table[:, 1::2] = torch.cos(position_ids * frequencies)\n",
    "    \n",
    "        # Registering the position enconding in state_dict but the its not included \n",
    "        # in named parameter as it is not trainable\n",
    "        self.register_buffer(\"positional_encoding_table\", positional_encoding_table)\n",
    "        \n",
    "    \n",
    "    def forward(self, embeddings_batch):\n",
    "        \"\"\"\n",
    "        embeddings_batch shape = (batch size, seq_length, dmodel)\n",
    "        positional_encoding_table shape = (max_seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        assert embeddings_batch.ndim == 3, \\\n",
    "        f\"Embeddings batch should have dimension of 3 but got {embeddings_batch.ndim}\"\n",
    "        assert embeddings_batch.size()[-1] == self.positional_encoding_table.size()[-1], \\\n",
    "        f\"Embedding batch shape and positional_encoding_table shape should match, expected Embedding batch shape : {embeddings_batch.shape[-1]} while positional_encoding_table shape : {positional_encoding_table[-1]}\"\n",
    "        \n",
    "        # Get encodings for the given input sequence length\n",
    "        pos_encodings = self.positional_encoding_table[:embeddings_batch.shape[1]] # Choose only seq_length out of max_seq_length\n",
    "        \n",
    "        # Final output \n",
    "        out = embeddings_batch + pos_encodings\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01650a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:51.896754Z",
     "iopub.status.busy": "2024-01-20T05:10:51.895962Z",
     "iopub.status.idle": "2024-01-20T05:10:51.937423Z",
     "shell.execute_reply": "2024-01-20T05:10:51.936509Z"
    },
    "papermill": {
     "duration": 0.056944,
     "end_time": "2024-01-20T05:10:51.939454",
     "exception": false,
     "start_time": "2024-01-20T05:10:51.882510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "dmodel = dk*h\n",
    "net = PositionalEncoding(dmodel)\n",
    "print(net)\n",
    "# summary(net, input_size=(100, 512))  # Can't visualize since params are none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a585130",
   "metadata": {
    "papermill": {
     "duration": 0.012929,
     "end_time": "2024-01-20T05:10:52.016597",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.003668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding positionwise FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a75c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.043189Z",
     "iopub.status.busy": "2024-01-20T05:10:52.042839Z",
     "iopub.status.idle": "2024-01-20T05:10:52.050316Z",
     "shell.execute_reply": "2024-01-20T05:10:52.049484Z"
    },
    "papermill": {
     "duration": 0.02334,
     "end_time": "2024-01-20T05:10:52.052497",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.029157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff, pdropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "        self.W1 = nn.Linear(dmodel, dff)      # Intermediate layer\n",
    "        self.W2 = nn.Linear(dff, dmodel)    # Output layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform Feedforward calculation\n",
    "        \n",
    "        x shape = (B - batch size, S/T - max token sequence length, D- model dimension).\n",
    "        \"\"\"\n",
    "        out = self.W2(self.relu(self.dropout(self.W1(x))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c44f3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.078597Z",
     "iopub.status.busy": "2024-01-20T05:10:52.077830Z",
     "iopub.status.idle": "2024-01-20T05:10:52.116692Z",
     "shell.execute_reply": "2024-01-20T05:10:52.115520Z"
    },
    "papermill": {
     "duration": 0.054085,
     "end_time": "2024-01-20T05:10:52.118880",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.064795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 2, 2048]       1,050,624\n",
      "           Dropout-2              [-1, 2, 2048]               0\n",
      "              ReLU-3              [-1, 2, 2048]               0\n",
      "            Linear-4               [-1, 2, 512]       1,049,088\n",
      "================================================================\n",
      "Total params: 2,099,712\n",
      "Trainable params: 2,099,712\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 8.01\n",
      "Estimated Total Size (MB): 8.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "dmodel = dk*h\n",
    "dff = dmodel * 4\n",
    "net = PositionwiseFeedForward(dmodel, dff)\n",
    "print(net)\n",
    "summary(net, input_size=(2, 512))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fdad7",
   "metadata": {
    "papermill": {
     "duration": 0.012417,
     "end_time": "2024-01-20T05:10:52.143783",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.131366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b03839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.171208Z",
     "iopub.status.busy": "2024-01-20T05:10:52.170866Z",
     "iopub.status.idle": "2024-01-20T05:10:52.185363Z",
     "shell.execute_reply": "2024-01-20T05:10:52.184486Z"
    },
    "papermill": {
     "duration": 0.030598,
     "end_time": "2024-01-20T05:10:52.187284",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.156686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a\n",
    "    \n",
    "    Here we are clubbing all the linear layers together and duplicating the inputs and \n",
    "    then performing matrix multiplications\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WK = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WV = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "            \n",
    "    def forward(self, query, key, val, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \n",
    "        X has a size of (batch_size, seq_length, d_model)\n",
    "        Wq, Wk, and Wv have a size of (d_model, d_model)\n",
    "        \n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, S, dmodel)\n",
    "        key shape = (B, S, dmodel)\n",
    "        val shape = (B, S, dmodel)\n",
    "        \"\"\"      \n",
    "        # Weight the queries\n",
    "        Q = self.WQ(query)     # shape -> (B, S, dmodel)\n",
    "        K = self.WK(key)       # shape -> (B, S, dmodel)\n",
    "        V = self.WV(val)       # shape -> (B, S, dmodel)\n",
    "        \n",
    "        # Separate last dimension to number of head and dk\n",
    "        batch_size = Q.size(0)   \n",
    "        Q = Q.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        K = K.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        V = V.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        \n",
    "        # each sequence is split across n_heads, with each head receiving seq_length tokens \n",
    "        # with d_key elements in each token instead of d_model.\n",
    "        Q = Q.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        K = K.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        V = V.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        \n",
    "        # dot product of Q and K\n",
    "        scaled_dot_product = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.dk)\n",
    "        \n",
    "        # fill those positions of product as (-1e10) where mask positions are 0\n",
    "        if mask is not None:\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        attention_prob = scaled_dot_product\n",
    "        \n",
    "        # Create head \n",
    "        head = torch.matmul(scaled_dot_product, V)  # shape -> (B, h, S, S) * (B, h, S, dk) = (B, h, S, dk)\n",
    "        # Prepare the head to pass it through output linear layer\n",
    "        head = head.permute(0, 2, 1, 3).contiguous()  # shape -> (B, S, h, dk)\n",
    "        # Concatenate the head together\n",
    "        head = head.view(batch_size, -1, self.h* self.dk)  # shape -> (B, S, (h*dk = dmodel))\n",
    "        # Pass through output layer\n",
    "        token_representation = self.WO(head)\n",
    "        return token_representation, attention_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713898cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.213122Z",
     "iopub.status.busy": "2024-01-20T05:10:52.212868Z",
     "iopub.status.idle": "2024-01-20T05:10:52.221284Z",
     "shell.execute_reply": "2024-01-20T05:10:52.220464Z"
    },
    "papermill": {
     "duration": 0.023595,
     "end_time": "2024-01-20T05:10:52.223179",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.199584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This building block in the encoder layer consists of the following\n",
    "    1. MultiHead Attention\n",
    "    2. Sublayer Logic\n",
    "    3. Positional FeedForward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h, dim_multiplier = 4, pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(dk, dv, h)\n",
    "        # Reference page 5 chapter 3.2.2 Multi-head attention\n",
    "        dmodel = dk*h\n",
    "        # Reference page 5 chapter 3.3 positionwise FeedForward\n",
    "        dff = dmodel * dim_multiplier\n",
    "        self.attn_norm = nn.LayerNorm(dmodel)\n",
    "        self.ff = PositionwiseFeedForward(dmodel, dff, pdropout=pdropout)\n",
    "        self.ff_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "    \n",
    "    def forward(self, src_inputs, src_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass as per page 3 chapter 3.1\n",
    "        \"\"\"\n",
    "        mha_out, attention_wts = self.attention(\n",
    "                                query = src_inputs, \n",
    "                                key = src_inputs, \n",
    "                                val = src_inputs, \n",
    "                                mask = src_mask)\n",
    "        \n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        # Actual paper design is the following\n",
    "        intermediate_out = self.attn_norm(src_inputs + self.dropout(mha_out))\n",
    "        \n",
    "        pff_out = self.ff(intermediate_out)\n",
    "        \n",
    "        # Perform Add Norm again\n",
    "        out = self.ff_norm(intermediate_out + self.dropout(pff_out))\n",
    "        return out, attention_wts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b76a65e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.248586Z",
     "iopub.status.busy": "2024-01-20T05:10:52.248293Z",
     "iopub.status.idle": "2024-01-20T05:10:52.285491Z",
     "shell.execute_reply": "2024-01-20T05:10:52.284498Z"
    },
    "papermill": {
     "duration": 0.053289,
     "end_time": "2024-01-20T05:10:52.288263",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.234974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): PositionwiseFeedForward(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 64\n",
    "dv = 64\n",
    "h = 8\n",
    "net = EncoderLayer(dk, dv, h)\n",
    "print(net)\n",
    "# summary(net, input_size=(2, 512), device=\"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3da1e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.315257Z",
     "iopub.status.busy": "2024-01-20T05:10:52.314957Z",
     "iopub.status.idle": "2024-01-20T05:10:52.322409Z",
     "shell.execute_reply": "2024-01-20T05:10:52.321462Z"
    },
    "papermill": {
     "duration": 0.023123,
     "end_time": "2024-01-20T05:10:52.324333",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.301210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dk, dv, h, num_encoders, dim_multiplier = 4, pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(dk, \n",
    "                         dv, \n",
    "                         h, \n",
    "                         dim_multiplier, \n",
    "                         pdropout) for _ in range(num_encoders)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, src_inputs, src_mask = None):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        src_inputs = (B - batch size, S/T - max token sequence length, D- model dimension)\n",
    "        \"\"\"\n",
    "        src_representation = src_inputs\n",
    "        \n",
    "        # Forward pass through encoder stack\n",
    "        for enc in self.encoder_layers:\n",
    "            src_representation, attention_wts = enc(src_representation, src_mask)\n",
    "            \n",
    "        self.attention_wts = attention_wts\n",
    "        return src_representation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a114e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.350684Z",
     "iopub.status.busy": "2024-01-20T05:10:52.350343Z",
     "iopub.status.idle": "2024-01-20T05:10:52.560667Z",
     "shell.execute_reply": "2024-01-20T05:10:52.559406Z"
    },
    "papermill": {
     "duration": 0.226035,
     "end_time": "2024-01-20T05:10:52.563062",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.337027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): PositionwiseFeedForward(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 64\n",
    "dv = 64\n",
    "h = 8\n",
    "num_encoders = 6\n",
    "dim_multiplier = 4\n",
    "pdropout=0.1\n",
    "net = Encoder(dk, dv, h, num_encoders, dim_multiplier, pdropout)\n",
    "print(net)\n",
    "# summary(net, input_size=(2, 512), device=\"cpu\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e6a04",
   "metadata": {
    "papermill": {
     "duration": 0.012465,
     "end_time": "2024-01-20T05:10:52.588427",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.575962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fd12e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.615771Z",
     "iopub.status.busy": "2024-01-20T05:10:52.615427Z",
     "iopub.status.idle": "2024-01-20T05:10:52.628150Z",
     "shell.execute_reply": "2024-01-20T05:10:52.627383Z"
    },
    "papermill": {
     "duration": 0.029601,
     "end_time": "2024-01-20T05:10:52.630293",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.600692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "                self, \n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                dim_multiplier = 4, \n",
    "                pdropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reference page 5 chapter 3.2.2 Multi-head attention\n",
    "        dmodel = dk*h\n",
    "        # Reference page 5 chapter 3.3 positionwise FeedForward\n",
    "        dff = dmodel * dim_multiplier\n",
    "        \n",
    "        # Masked Multi Head Attention\n",
    "        self.masked_attention = MultiHeadAttention(dk, dv, h)\n",
    "        self.masked_attn_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        # Multi head attention\n",
    "        self.attention = MultiHeadAttention(dk, dv, h)\n",
    "        self.attn_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        # Add position FeedForward Network\n",
    "        self.ff = PositionwiseFeedForward(dmodel, dff, pdropout=pdropout)\n",
    "        self.ff_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "    \n",
    "    def forward(self, target_inputs, src_inputs, target_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        target_inputs = embedded sequences    (batch_size, trg_seq_length, d_model)\n",
    "        src_inputs = embedded sequences       (batch_size, src_seq_length, d_model)\n",
    "        target_mask = mask for the sequences  (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        src_mask = mask for the sequences     (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        mmha_out, attention_wts = self.masked_attention(\n",
    "                                query = target_inputs, \n",
    "                                key = target_inputs, \n",
    "                                val = target_inputs, \n",
    "                                mask = target_mask)\n",
    "        \n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        # Actual paper design is the following\n",
    "        target_inputs = self.masked_attn_norm(target_inputs + self.dropout(mmha_out))\n",
    "        \n",
    "        # Inputs to the decoder attention is given as follows\n",
    "        # query = previous decoder layer\n",
    "        # key and val = output of encoder\n",
    "        # mask = src_mask\n",
    "        # Reference : page 5 chapter 3.2.3 point 1\n",
    "        mha_out, attention_wts = self.attention(\n",
    "                                query = target_inputs, \n",
    "                                key = src_inputs, \n",
    "                                val = src_inputs, \n",
    "                                mask = src_mask)\n",
    "        target_inputs = self.attn_norm(target_inputs + self.dropout(mha_out))\n",
    "        \n",
    "        pff_out = self.ff(target_inputs)\n",
    "        # Perform Add Norm again\n",
    "        out = self.ff_norm(target_inputs + self.dropout(pff_out))\n",
    "        return out, attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f70b15d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.657098Z",
     "iopub.status.busy": "2024-01-20T05:10:52.656826Z",
     "iopub.status.idle": "2024-01-20T05:10:52.705303Z",
     "shell.execute_reply": "2024-01-20T05:10:52.704406Z"
    },
    "papermill": {
     "duration": 0.063864,
     "end_time": "2024-01-20T05:10:52.707303",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.643439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer(\n",
      "  (masked_attention): MultiHeadAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (masked_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MultiHeadAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): PositionwiseFeedForward(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 64\n",
    "dv = 64\n",
    "h = 8\n",
    "net = DecoderLayer(dk, dv, h)\n",
    "print(net)\n",
    "# summary(net, input_size=(2, 512), device=\"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1d75e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.736020Z",
     "iopub.status.busy": "2024-01-20T05:10:52.735733Z",
     "iopub.status.idle": "2024-01-20T05:10:52.744360Z",
     "shell.execute_reply": "2024-01-20T05:10:52.743394Z"
    },
    "papermill": {
     "duration": 0.025253,
     "end_time": "2024-01-20T05:10:52.746496",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.721243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "                self, \n",
    "                dk, \n",
    "                dv, \n",
    "                h, \n",
    "                num_decoders, \n",
    "                dim_multiplier = 4, \n",
    "                pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(dk, \n",
    "                         dv, \n",
    "                         h, \n",
    "                         dim_multiplier, \n",
    "                         pdropout) for _ in range(num_decoders)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, target_inputs, src_inputs, target_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        target_inputs = embedded sequences    (batch_size, trg_seq_length, d_model)\n",
    "        src_inputs = embedded sequences       (batch_size, src_seq_length, d_model)\n",
    "        target_mask = mask for the sequences  (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        src_mask = mask for the sequences     (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        target_representation = target_inputs\n",
    "        \n",
    "        # Forward pass through decoder stack\n",
    "        for layer in self.decoder_layers:\n",
    "            target_representation = layer(\n",
    "                                    target_representation,\n",
    "                                    src_inputs, \n",
    "                                    target_mask,\n",
    "                                    src_mask)\n",
    "        return target_representation\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "160a264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:52.772747Z",
     "iopub.status.busy": "2024-01-20T05:10:52.772439Z",
     "iopub.status.idle": "2024-01-20T05:10:53.006897Z",
     "shell.execute_reply": "2024-01-20T05:10:53.005354Z"
    },
    "papermill": {
     "duration": 0.249985,
     "end_time": "2024-01-20T05:10:53.009081",
     "exception": false,
     "start_time": "2024-01-20T05:10:52.759096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): PositionwiseFeedForward(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 64\n",
    "dv = 64\n",
    "h = 8\n",
    "num_decoders = 6\n",
    "dim_multiplier = 4\n",
    "pdropout=0.1\n",
    "net = Decoder(dk, dv, h, num_decoders, dim_multiplier, pdropout)\n",
    "print(net)\n",
    "# summary(net, input_size=([[2, 10, 512], [2, 10, 512], [2, 1, 10, 10], [2, 1, 1, 10]]), device=\"cpu\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c67fe3",
   "metadata": {
    "papermill": {
     "duration": 0.012816,
     "end_time": "2024-01-20T05:10:53.034958",
     "exception": false,
     "start_time": "2024-01-20T05:10:53.022142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Adding all up to construct the complete model for language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e3a4b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:53.062594Z",
     "iopub.status.busy": "2024-01-20T05:10:53.062219Z",
     "iopub.status.idle": "2024-01-20T05:10:53.080284Z",
     "shell.execute_reply": "2024-01-20T05:10:53.079341Z"
    },
    "papermill": {
     "duration": 0.034129,
     "end_time": "2024-01-20T05:10:53.082255",
     "exception": false,
     "start_time": "2024-01-20T05:10:53.048126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                src_vocab_size,\n",
    "                target_vocab_size,\n",
    "                num_encoders,\n",
    "                num_decoders,\n",
    "                dim_multiplier = 4, \n",
    "                pdropout=0.1,\n",
    "                device = \"cpu\"\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        dmodel = dk*h\n",
    "        \n",
    "        # Modules required to build Encoder\n",
    "        self.src_embeddings = Embedding(src_vocab_size, dmodel)\n",
    "        self.src_positional_encoding = PositionalEncoding(\n",
    "                                        dmodel,\n",
    "                                        max_seq_length = src_vocab_size,\n",
    "                                        pdropout = pdropout\n",
    "                                        )\n",
    "        self.encoder = Encoder(\n",
    "                                dk, \n",
    "                                dv, \n",
    "                                h, \n",
    "                                num_encoders, \n",
    "                                dim_multiplier=dim_multiplier, \n",
    "                                pdropout=pdropout)\n",
    "        \n",
    "        # Modules required to build Decoder\n",
    "        self.target_embeddings = Embedding(target_vocab_size, dmodel)\n",
    "        self.target_positional_encoding = PositionalEncoding(\n",
    "                                        dmodel,\n",
    "                                        max_seq_length = target_vocab_size,\n",
    "                                        pdropout = pdropout\n",
    "                                        )\n",
    "        self.decoder = Decoder(\n",
    "                                dk, \n",
    "                                dv, \n",
    "                                h, \n",
    "                                num_decoders,  \n",
    "                                dim_multiplier=4, \n",
    "                                pdropout=0.1)\n",
    "        \n",
    "        # Final output \n",
    "        self.linear = nn.Linear(dmodel, target_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "        self.init_params()  \n",
    "    \n",
    "    # This part wasn't mentioned in the paper, but it's super important!\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        xavier has tremendous impact! I didn't expect\n",
    "        that the model's perf, with normalization layers, \n",
    "        is so dependent on the choice of weight initialization.\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def make_src_mask(self, src, src_pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: raw sequences with padding        (batch_size, seq_length) \n",
    "            src_pad_idx(int): index where the token need not be attended\n",
    "\n",
    "        Returns:\n",
    "            src_mask: mask for each sequence            (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        # assign 1 to tokens that need attended to and 0 to padding tokens, \n",
    "        # then add 2 dimensions\n",
    "        src_mask = (src != src_pad_idx).view(batch_size, 1, 1, -1)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_target_mask(self, target, target_pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target:  raw sequences with padding        (batch_size, seq_length)     \n",
    "            target_pad_idx(int): index where the token need not be attended\n",
    "\n",
    "        Returns:\n",
    "            target_mask: mask for each sequence   (batch_size, 1, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_length = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        # assign True to tokens that need attended to and False to padding tokens, then add 2 dimensions\n",
    "        target_mask = (trg != target_pad_idx).view(batch_size, 1, 1, -1) # (batch_size, 1, 1, seq_length)\n",
    "\n",
    "        # generate subsequent mask\n",
    "        trg_sub_mask = torch.tril(torch.ones((seq_length, seq_length), device=self.device)).bool() # (batch_size, 1, seq_length, seq_length)\n",
    "\n",
    "        # bitwise \"and\" operator | 0 & 0 = 0, 1 & 1 = 1, 1 & 0 = 0\n",
    "        target_mask = target_mask & trg_sub_mask\n",
    "\n",
    "        return target_mask\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        src_token_ids_batch, \n",
    "        target_token_ids_batch, \n",
    "        src_pad_idx, \n",
    "        target_pad_idx):\n",
    "        \n",
    "        # create source and target masks     \n",
    "        src_mask = self.make_src_mask(\n",
    "                        src_token_ids_batch, \n",
    "                        src_pad_idx) # (batch_size, 1, 1, src_seq_length)\n",
    "        target_mask = self.make_target_mask(\n",
    "                        target_token_ids_batch, \n",
    "                        target_pad_idx) # (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        \n",
    "        # Create embeddings\n",
    "        src_representations = self.src_embeddings(src_token_ids_batch)\n",
    "        src_representations = self.src_positional_encoding(src_representations)\n",
    "        \n",
    "        target_representations = self.target_embeddings(target_token_ids_batch)\n",
    "        target_representations = self.target_positional_encoding(target_representations)\n",
    "        \n",
    "        # Encode \n",
    "        encoded_src = self.encoder(src_representations, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_output = self.decoder(\n",
    "                                target_representations, \n",
    "                                encoded_src, \n",
    "                                target_mask, \n",
    "                                src_mask)\n",
    "        \n",
    "        # Post processing\n",
    "        out = self.linear(decoded_output)\n",
    "        # Output \n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b45177a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T05:10:53.109140Z",
     "iopub.status.busy": "2024-01-20T05:10:53.108836Z",
     "iopub.status.idle": "2024-01-20T05:10:53.974521Z",
     "shell.execute_reply": "2024-01-20T05:10:53.973325Z"
    },
    "papermill": {
     "duration": 0.881579,
     "end_time": "2024-01-20T05:10:53.976578",
     "exception": false,
     "start_time": "2024-01-20T05:10:53.094999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embeddings): Embedding(\n",
      "    (embedding_lookup): Embedding(1000, 512)\n",
      "  )\n",
      "  (src_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (target_embeddings): Embedding(\n",
      "    (embedding_lookup): Embedding(1000, 512)\n",
      "  )\n",
      "  (target_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (masked_attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (masked_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (W2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 64\n",
    "dv = 64\n",
    "h = 8\n",
    "src_vocab_size = 1000\n",
    "target_vocab_size = 1000\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "dim_multiplier = 4\n",
    "pdropout=0.1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net = Transformer(\n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                src_vocab_size,\n",
    "                target_vocab_size,\n",
    "                num_encoders,\n",
    "                num_decoders,\n",
    "                dim_multiplier, \n",
    "                pdropout,\n",
    "                device = device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d597d0c",
   "metadata": {
    "papermill": {
     "duration": 0.012664,
     "end_time": "2024-01-20T05:10:54.002803",
     "exception": false,
     "start_time": "2024-01-20T05:10:53.990139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2802718,
     "sourceId": 4853240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.425917,
   "end_time": "2024-01-20T05:10:54.741408",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-20T05:10:29.315491",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
