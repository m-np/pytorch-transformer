{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43195f83",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.015551,
     "end_time": "2024-01-20T21:45:29.868673",
     "exception": false,
     "start_time": "2024-01-20T21:45:29.853122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training model for Machine Translation\n",
    "\n",
    "Library Imports for the jupyter notebook. We are refering to this [blog](https://medium.com/@hunter-j-phillips/putting-it-all-together-the-implemented-transformer-bfb11ac1ddfehttps://medium.com/@hunter-j-phillips/putting-it-all-together-the-implemented-transformer-bfb11ac1ddfe) to understand attention network in depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d55cdba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:29.901668Z",
     "iopub.status.busy": "2024-01-20T21:45:29.901090Z",
     "iopub.status.idle": "2024-01-20T21:45:54.971945Z",
     "shell.execute_reply": "2024-01-20T21:45:54.970822Z"
    },
    "papermill": {
     "duration": 25.090693,
     "end_time": "2024-01-20T21:45:54.974717",
     "exception": false,
     "start_time": "2024-01-20T21:45:29.884024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q portalocker\n",
    "\n",
    "# importing required libraries\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "# load and build datasets\n",
    "import torchtext\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import portalocker\n",
    "\n",
    "# visualization packages\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4aa89",
   "metadata": {
    "papermill": {
     "duration": 0.020901,
     "end_time": "2024-01-20T21:45:55.011036",
     "exception": false,
     "start_time": "2024-01-20T21:45:54.990135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading the dataframe and converting it into iterable for consuming in pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b756de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.044147Z",
     "iopub.status.busy": "2024-01-20T21:45:55.043253Z",
     "iopub.status.idle": "2024-01-20T21:45:55.049456Z",
     "shell.execute_reply": "2024-01-20T21:45:55.048018Z"
    },
    "papermill": {
     "duration": 0.02579,
     "end_time": "2024-01-20T21:45:55.051969",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.026179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e72b66e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.084928Z",
     "iopub.status.busy": "2024-01-20T21:45:55.084396Z",
     "iopub.status.idle": "2024-01-20T21:45:55.310499Z",
     "shell.execute_reply": "2024-01-20T21:45:55.309001Z"
    },
    "papermill": {
     "duration": 0.246801,
     "end_time": "2024-01-20T21:45:55.313468",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.066667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two young, White males are outside near many b...</td>\n",
       "      <td>Zwei junge weiße Männer sind im Freien in der ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>Mehrere Männer mit Schutzhelmen bedienen ein A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A little girl climbing into a wooden playhouse.</td>\n",
       "      <td>Ein kleines Mädchen klettert in ein Spielhaus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man in a blue shirt is standing on a ladder ...</td>\n",
       "      <td>Ein Mann in einem blauen Hemd steht auf einer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two men are at the stove preparing food.</td>\n",
       "      <td>Zwei Männer stehen am Herd und bereiten Essen zu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  Two young, White males are outside near many b...   \n",
       "1  Several men in hard hats are operating a giant...   \n",
       "2    A little girl climbing into a wooden playhouse.   \n",
       "3  A man in a blue shirt is standing on a ladder ...   \n",
       "4           Two men are at the stove preparing food.   \n",
       "\n",
       "                                              german  \n",
       "0  Zwei junge weiße Männer sind im Freien in der ...  \n",
       "1  Mehrere Männer mit Schutzhelmen bedienen ein A...  \n",
       "2  Ein kleines Mädchen klettert in ein Spielhaus ...  \n",
       "3  Ein Mann in einem blauen Hemd steht auf einer ...  \n",
       "4  Zwei Männer stehen am Herd und bereiten Essen zu.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/machine-translation-dataset-de-en/translation_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb89eb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.347885Z",
     "iopub.status.busy": "2024-01-20T21:45:55.346732Z",
     "iopub.status.idle": "2024-01-20T21:45:55.355613Z",
     "shell.execute_reply": "2024-01-20T21:45:55.354181Z"
    },
    "papermill": {
     "duration": 0.029536,
     "end_time": "2024-01-20T21:45:55.358391",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.328855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bd5eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.390818Z",
     "iopub.status.busy": "2024-01-20T21:45:55.390392Z",
     "iopub.status.idle": "2024-01-20T21:45:55.409057Z",
     "shell.execute_reply": "2024-01-20T21:45:55.407813Z"
    },
    "papermill": {
     "duration": 0.038482,
     "end_time": "2024-01-20T21:45:55.412206",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.373724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform train - val split\n",
    "train_df=df.sample(frac=0.95,random_state=200)\n",
    "val_df=df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e279c9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.445691Z",
     "iopub.status.busy": "2024-01-20T21:45:55.445198Z",
     "iopub.status.idle": "2024-01-20T21:45:55.453060Z",
     "shell.execute_reply": "2024-01-20T21:45:55.451544Z"
    },
    "papermill": {
     "duration": 0.028163,
     "end_time": "2024-01-20T21:45:55.455565",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.427402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_iterable(sample_df):\n",
    "    \"\"\"\n",
    "    This converts pandas dataframe into list of tuples\n",
    "    consisting of (german sentences, english sentences).\n",
    "    \n",
    "    This iterable in used in our data preparation\n",
    "    \"\"\"\n",
    "    sample_iter = sample_df.to_dict(orient='records')\n",
    "    out_iter = [(dict1[\"german\"], dict1[\"english\"]) for dict1 in sample_iter]\n",
    "    print(f\"length of iterable: {len(out_iter)}\")\n",
    "    return out_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ed8134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.488580Z",
     "iopub.status.busy": "2024-01-20T21:45:55.488073Z",
     "iopub.status.idle": "2024-01-20T21:45:55.661533Z",
     "shell.execute_reply": "2024-01-20T21:45:55.659735Z"
    },
    "papermill": {
     "duration": 0.193407,
     "end_time": "2024-01-20T21:45:55.664385",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.470978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of iterable: 27550\n",
      "length of iterable: 1450\n"
     ]
    }
   ],
   "source": [
    "train_iter = create_iterable(train_df)\n",
    "val_iter = create_iterable(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e20416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.698107Z",
     "iopub.status.busy": "2024-01-20T21:45:55.697163Z",
     "iopub.status.idle": "2024-01-20T21:45:55.724202Z",
     "shell.execute_reply": "2024-01-20T21:45:55.722864Z"
    },
    "papermill": {
     "duration": 0.047368,
     "end_time": "2024-01-20T21:45:55.727030",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.679662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man in an orange hat starring at something.</td>\n",
       "      <td>Ein Mann mit einem orangefarbenen Hut, der etw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Boston Terrier is running on lush green gras...</td>\n",
       "      <td>Ein Boston Terrier läuft über saftig-grünes Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A girl in karate uniform breaking a stick with...</td>\n",
       "      <td>Ein Mädchen in einem Karateanzug bricht einen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five people wearing winter jackets and helmets...</td>\n",
       "      <td>Fünf Leute in Winterjacken und mit Helmen steh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People are fixing the roof of a house.</td>\n",
       "      <td>Leute Reparieren das Dach eines Hauses.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0      A man in an orange hat starring at something.   \n",
       "1  A Boston Terrier is running on lush green gras...   \n",
       "2  A girl in karate uniform breaking a stick with...   \n",
       "3  Five people wearing winter jackets and helmets...   \n",
       "4             People are fixing the roof of a house.   \n",
       "\n",
       "                                              german  \n",
       "0  Ein Mann mit einem orangefarbenen Hut, der etw...  \n",
       "1  Ein Boston Terrier läuft über saftig-grünes Gr...  \n",
       "2  Ein Mädchen in einem Karateanzug bricht einen ...  \n",
       "3  Fünf Leute in Winterjacken und mit Helmen steh...  \n",
       "4            Leute Reparieren das Dach eines Hauses.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/machine-translation-dataset-de-en/translation_test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a0defd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.760661Z",
     "iopub.status.busy": "2024-01-20T21:45:55.760239Z",
     "iopub.status.idle": "2024-01-20T21:45:55.773070Z",
     "shell.execute_reply": "2024-01-20T21:45:55.771596Z"
    },
    "papermill": {
     "duration": 0.032549,
     "end_time": "2024-01-20T21:45:55.775871",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.743322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of iterable: 1000\n"
     ]
    }
   ],
   "source": [
    "test_iter = create_iterable(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f4a81",
   "metadata": {
    "papermill": {
     "duration": 0.015812,
     "end_time": "2024-01-20T21:45:55.808235",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.792423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Vocab from data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7295284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.841738Z",
     "iopub.status.busy": "2024-01-20T21:45:55.841208Z",
     "iopub.status.idle": "2024-01-20T21:45:55.849312Z",
     "shell.execute_reply": "2024-01-20T21:45:55.847994Z"
    },
    "papermill": {
     "duration": 0.028125,
     "end_time": "2024-01-20T21:45:55.852000",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.823875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    \"\"\"\n",
    "    Load the German and English tokenizers provided by spaCy.\n",
    "\n",
    "    Returns:\n",
    "        spacy_de:     German tokenizer\n",
    "        spacy_en:     English tokenizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except OSError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    print(\"Loaded English and German tokenizers.\")\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659b3fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.951259Z",
     "iopub.status.busy": "2024-01-20T21:45:55.950808Z",
     "iopub.status.idle": "2024-01-20T21:45:55.957154Z",
     "shell.execute_reply": "2024-01-20T21:45:55.955718Z"
    },
    "papermill": {
     "duration": 0.092323,
     "end_time": "2024-01-20T21:45:55.959754",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.867431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str, tokenizer):\n",
    "  \"\"\"\n",
    "    Split a string into its tokens using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text:         string \n",
    "        tokenizer:    tokenizer for the language\n",
    "        \n",
    "    Returns:\n",
    "        tokenized list of strings       \n",
    "  \"\"\"\n",
    "  return [tok.text.lower() for tok in tokenizer.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b079ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:55.995698Z",
     "iopub.status.busy": "2024-01-20T21:45:55.995271Z",
     "iopub.status.idle": "2024-01-20T21:45:56.003099Z",
     "shell.execute_reply": "2024-01-20T21:45:56.001076Z"
    },
    "papermill": {
     "duration": 0.030565,
     "end_time": "2024-01-20T21:45:56.007402",
     "exception": false,
     "start_time": "2024-01-20T21:45:55.976837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter, tokenizer, index: int):\n",
    "  \"\"\"\n",
    "    Return the tokens for the appropriate language.\n",
    "\n",
    "    Args:\n",
    "        data_iter:    text here \n",
    "        tokenizer:    tokenizer for the language\n",
    "        index:        index of the language in the tuple | (de=0, en=1)\n",
    "        \n",
    "    Yields:\n",
    "        sequences based on index       \n",
    "  \"\"\"\n",
    "  for from_tuple in data_iter:\n",
    "    yield tokenizer(from_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1361b3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:56.043338Z",
     "iopub.status.busy": "2024-01-20T21:45:56.042897Z",
     "iopub.status.idle": "2024-01-20T21:45:56.054314Z",
     "shell.execute_reply": "2024-01-20T21:45:56.053042Z"
    },
    "papermill": {
     "duration": 0.032614,
     "end_time": "2024-01-20T21:45:56.056983",
     "exception": false,
     "start_time": "2024-01-20T21:45:56.024369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(\n",
    "                    spacy_de, \n",
    "                    spacy_en, \n",
    "                    train_iter, \n",
    "                    val_iter, \n",
    "                    test_iter, \n",
    "                    min_freq: int = 2):\n",
    "  \n",
    "    def tokenize_de(text: str):\n",
    "        \"\"\"\n",
    "          Call the German tokenizer.\n",
    "\n",
    "          Args:\n",
    "              text:         string \n",
    "              min_freq:     minimum frequency needed to include a word in the vocabulary\n",
    "\n",
    "          Returns:\n",
    "              tokenized list of strings       \n",
    "        \"\"\"\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text: str):\n",
    "        \"\"\"\n",
    "          Call the English tokenizer.\n",
    "\n",
    "          Args:\n",
    "              text:         string \n",
    "\n",
    "          Returns:\n",
    "              tokenized list of strings       \n",
    "        \"\"\"\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary...\")\n",
    "\n",
    "#     # load train, val, and test data pipelines\n",
    "#     train, val, test = datasets.IWSLT2016(language_pair=(\"de\", \"en\"))\n",
    "    train = train_iter\n",
    "    val = val_iter\n",
    "    test = test_iter\n",
    "\n",
    "    # generate source vocabulary\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0), # tokens for each German sentence (index 0)\n",
    "        min_freq=min_freq, \n",
    "        specials=[\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary...\")\n",
    "\n",
    "    # generate target vocabulary\n",
    "    vocab_trg = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1), # tokens for each English sentence (index 1)\n",
    "        min_freq=2, # \n",
    "        specials=[\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    # set default token for out-of-vocabulary words (OOV)\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_trg.set_default_index(vocab_trg[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bf7261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:56.092069Z",
     "iopub.status.busy": "2024-01-20T21:45:56.091615Z",
     "iopub.status.idle": "2024-01-20T21:45:56.114746Z",
     "shell.execute_reply": "2024-01-20T21:45:56.113470Z"
    },
    "papermill": {
     "duration": 0.044051,
     "end_time": "2024-01-20T21:45:56.117446",
     "exception": false,
     "start_time": "2024-01-20T21:45:56.073395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vocab(spacy_de, spacy_en, train_iter, val_iter, test_iter, min_freq: int = 2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        spacy_de:     German tokenizer\n",
    "        spacy_en:     English tokenizer\n",
    "        min_freq:     minimum frequency needed to include a word in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        vocab_src:    German vocabulary\n",
    "        vocab_trg:     English vocabulary       \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        # build the German/English vocabulary if it does not exist\n",
    "        vocab_src, vocab_trg = build_vocabulary(spacy_de, \n",
    "                                                spacy_en, \n",
    "                                                train_iter, \n",
    "                                                val_iter, \n",
    "                                                test_iter, \n",
    "                                                min_freq)\n",
    "        # save it to a file\n",
    "        torch.save((vocab_src, vocab_trg), \"vocab.pt\")\n",
    "    else:\n",
    "        # load the vocab if it exists\n",
    "        vocab_src, vocab_trg = torch.load(\"vocab.pt\")\n",
    "\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(\"\\tSource:\", len(vocab_src))\n",
    "    print(\"\\tTarget:\", len(vocab_trg))\n",
    "    return vocab_src, vocab_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6148425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:45:56.152616Z",
     "iopub.status.busy": "2024-01-20T21:45:56.152207Z",
     "iopub.status.idle": "2024-01-20T21:46:19.283897Z",
     "shell.execute_reply": "2024-01-20T21:46:19.282001Z"
    },
    "papermill": {
     "duration": 23.15317,
     "end_time": "2024-01-20T21:46:19.287461",
     "exception": false,
     "start_time": "2024-01-20T21:45:56.134291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.24.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Loaded English and German tokenizers.\n"
     ]
    }
   ],
   "source": [
    "# global variables used later in the script\n",
    "spacy_de, spacy_en = load_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1ace46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:19.325256Z",
     "iopub.status.busy": "2024-01-20T21:46:19.324671Z",
     "iopub.status.idle": "2024-01-20T21:46:25.775998Z",
     "shell.execute_reply": "2024-01-20T21:46:25.774161Z"
    },
    "papermill": {
     "duration": 6.473234,
     "end_time": "2024-01-20T21:46:25.778802",
     "exception": false,
     "start_time": "2024-01-20T21:46:19.305568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building German Vocabulary...\n",
      "Building English Vocabulary...\n",
      "Finished.\n",
      "Vocabulary sizes:\n",
      "\tSource: 7983\n",
      "\tTarget: 5979\n"
     ]
    }
   ],
   "source": [
    "vocab_src, vocab_trg = load_vocab(\n",
    "                            spacy_de, \n",
    "                            spacy_en, \n",
    "                            train_iter, \n",
    "                            val_iter, \n",
    "                            test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6508ae8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:25.816297Z",
     "iopub.status.busy": "2024-01-20T21:46:25.815891Z",
     "iopub.status.idle": "2024-01-20T21:46:25.821891Z",
     "shell.execute_reply": "2024-01-20T21:46:25.820344Z"
    },
    "papermill": {
     "duration": 0.027748,
     "end_time": "2024-01-20T21:46:25.824243",
     "exception": false,
     "start_time": "2024-01-20T21:46:25.796495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_IDX = vocab_trg['<bos>']\n",
    "EOS_IDX = vocab_trg['<eos>']\n",
    "PAD_IDX = vocab_trg['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876abc2",
   "metadata": {
    "papermill": {
     "duration": 0.017493,
     "end_time": "2024-01-20T21:46:25.858846",
     "exception": false,
     "start_time": "2024-01-20T21:46:25.841353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perform data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cb9383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:25.894779Z",
     "iopub.status.busy": "2024-01-20T21:46:25.894287Z",
     "iopub.status.idle": "2024-01-20T21:46:25.903368Z",
     "shell.execute_reply": "2024-01-20T21:46:25.902444Z"
    },
    "papermill": {
     "duration": 0.030613,
     "end_time": "2024-01-20T21:46:25.906413",
     "exception": false,
     "start_time": "2024-01-20T21:46:25.875800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_process(raw_data):\n",
    "    \"\"\"\n",
    "    Process raw sentences by tokenizing and converting to integers based on \n",
    "    the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        raw_data:     German-English sentence pairs \n",
    "    Returns:\n",
    "        data:         tokenized data converted to index based on vocabulary   \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # loop through each sentence pair\n",
    "    for (raw_de, raw_en) in tqdm(raw_data):\n",
    "        de_tensor_ = []\n",
    "        # tokenize the sentence and convert each word to an integers\n",
    "        for token in spacy_de.tokenizer(raw_de):\n",
    "            de_tensor_.append(vocab_src[token.text.lower()])\n",
    "            \n",
    "        en_tensor_ = []\n",
    "        # tokenize the sentence and convert each word to an integers\n",
    "        for token in spacy_en.tokenizer(raw_en):\n",
    "            en_tensor_.append(vocab_trg[token.text.lower()])\n",
    "            \n",
    "        de_tensor_ = torch.tensor(de_tensor_, dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor(en_tensor_, dtype=torch.long)\n",
    "        # append tensor representations\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd2e678c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:25.943550Z",
     "iopub.status.busy": "2024-01-20T21:46:25.943060Z",
     "iopub.status.idle": "2024-01-20T21:46:31.624437Z",
     "shell.execute_reply": "2024-01-20T21:46:31.623004Z"
    },
    "papermill": {
     "duration": 5.704395,
     "end_time": "2024-01-20T21:46:31.628722",
     "exception": false,
     "start_time": "2024-01-20T21:46:25.924327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27550/27550 [00:05<00:00, 5274.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: 27550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [00:00<00:00, 5567.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data shape: 1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 5793.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# processed data\n",
    "train_data = data_process(train_iter)\n",
    "print(f\"Train data shape: {len(train_data)}\")\n",
    "val_data = data_process(val_iter)\n",
    "print(f\"Val data shape: {len(val_data)}\")\n",
    "test_data = data_process(test_iter)\n",
    "print(f\"Test data shape: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2c934",
   "metadata": {
    "papermill": {
     "duration": 0.023583,
     "end_time": "2024-01-20T21:46:31.677967",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.654384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49412dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:31.728885Z",
     "iopub.status.busy": "2024-01-20T21:46:31.728397Z",
     "iopub.status.idle": "2024-01-20T21:46:31.739091Z",
     "shell.execute_reply": "2024-01-20T21:46:31.737943Z"
    },
    "papermill": {
     "duration": 0.040049,
     "end_time": "2024-01-20T21:46:31.741642",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.701593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"\n",
    "    Process indexed-sequences by adding <bos>, <eos>, and <pad> tokens.\n",
    "\n",
    "    Args:\n",
    "        data_batch:     German-English indexed-sentence pairs\n",
    "\n",
    "    Returns:\n",
    "        two batches:    one for German and one for English\n",
    "    \"\"\"\n",
    "    de_batch, en_batch = [], []\n",
    "\n",
    "    # for each sentence\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        # add <bos> and <eos> indices before and after the sentence\n",
    "        de_temp = torch.cat([torch.tensor([BOS_IDX]), \n",
    "                             de_item, \n",
    "                             torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "        en_temp = torch.cat([torch.tensor([BOS_IDX]), \n",
    "                             en_item, \n",
    "                             torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "\n",
    "        # add padding\n",
    "        de_batch.append(pad(de_temp,(0, # dimension to pad\n",
    "                                MAX_PADDING - len(de_temp), # amount of padding to add\n",
    "                              ),value=PAD_IDX,))\n",
    "\n",
    "        # add padding\n",
    "        en_batch.append(pad(en_temp,(0, # dimension to pad\n",
    "                                MAX_PADDING - len(en_temp), # amount of padding to add\n",
    "                              ),\n",
    "                              value=PAD_IDX,))\n",
    "\n",
    "    return torch.stack(de_batch), torch.stack(en_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924506e",
   "metadata": {
    "papermill": {
     "duration": 0.024321,
     "end_time": "2024-01-20T21:46:31.790488",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.766167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating DataLoaders for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f10dacdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:31.841972Z",
     "iopub.status.busy": "2024-01-20T21:46:31.841499Z",
     "iopub.status.idle": "2024-01-20T21:46:31.855748Z",
     "shell.execute_reply": "2024-01-20T21:46:31.854610Z"
    },
    "papermill": {
     "duration": 0.043001,
     "end_time": "2024-01-20T21:46:31.858033",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.815032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_PADDING = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter = DataLoader(\n",
    "                to_map_style_dataset(train_data), \n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True, \n",
    "                drop_last=True, \n",
    "                collate_fn=generate_batch)\n",
    "\n",
    "valid_iter = DataLoader(\n",
    "                to_map_style_dataset(val_data),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True, \n",
    "                drop_last=True, \n",
    "                collate_fn=generate_batch)\n",
    "\n",
    "test_iter = DataLoader(\n",
    "                to_map_style_dataset(test_data), \n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True, \n",
    "                drop_last=True, \n",
    "                collate_fn=generate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de754",
   "metadata": {
    "papermill": {
     "duration": 0.023821,
     "end_time": "2024-01-20T21:46:31.905267",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.881446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Attention Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ed9a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:31.954736Z",
     "iopub.status.busy": "2024-01-20T21:46:31.954227Z",
     "iopub.status.idle": "2024-01-20T21:46:31.959971Z",
     "shell.execute_reply": "2024-01-20T21:46:31.958439Z"
    },
    "papermill": {
     "duration": 0.033641,
     "end_time": "2024-01-20T21:46:31.962341",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.928700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d91de",
   "metadata": {
    "papermill": {
     "duration": 0.024979,
     "end_time": "2024-01-20T21:46:32.010070",
     "exception": false,
     "start_time": "2024-01-20T21:46:31.985091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create submodules for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0df5229d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.059328Z",
     "iopub.status.busy": "2024-01-20T21:46:32.058672Z",
     "iopub.status.idle": "2024-01-20T21:46:32.067532Z",
     "shell.execute_reply": "2024-01-20T21:46:32.066497Z"
    },
    "papermill": {
     "duration": 0.03676,
     "end_time": "2024-01-20T21:46:32.070227",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.033467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding lookup table which is used by the positional embedding block.\n",
    "    Embedding lookup table is shared across input and output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, dmodel):\n",
    "        \"\"\"\n",
    "        Embedding lookup needs a vocab size and model dimension size matrix for \n",
    "        creating lookups\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_lookup = nn.Embedding(vocab_size, dmodel)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dmodel = dmodel\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        For a given token lookup the embedding vector\n",
    "        \n",
    "        As per the paper, we also multiply the embedding vector with sqrt of dmodel \n",
    "        \"\"\"\n",
    "        assert token_ids.ndim == 2, \\\n",
    "        f'Expected: (batch size, max token sequence length), got {token_ids.shape}'\n",
    "        \n",
    "        embedding_vector = self.embedding_lookup(token_ids)\n",
    "        \n",
    "        return embedding_vector * math.sqrt(self.dmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d0b526b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.120778Z",
     "iopub.status.busy": "2024-01-20T21:46:32.120328Z",
     "iopub.status.idle": "2024-01-20T21:46:32.132698Z",
     "shell.execute_reply": "2024-01-20T21:46:32.131660Z"
    },
    "papermill": {
     "duration": 0.040295,
     "end_time": "2024-01-20T21:46:32.135408",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.095113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dmodel, max_seq_length = 5000, pdropout = 0.1,):\n",
    "        \"\"\"\n",
    "        dmodel(int): model dimensions\n",
    "        max_seq_length(int): Maximum input sequence length\n",
    "        pdropout(float): Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "        # Calculate frequencies\n",
    "        position_ids = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        # -ve sign is added because the exponents are inverted when you multiply position and frequencies\n",
    "        frequencies = torch.pow(10000, -torch.arange(0, dmodel, 2, dtype = torch.float)/ dmodel) \n",
    "        \n",
    "        # Create positional encoding table\n",
    "        positional_encoding_table = torch.zeros(max_seq_length, dmodel)\n",
    "        # Fill the table with even entries with sin and odd entries with cosine\n",
    "        positional_encoding_table[:, 0::2] = torch.sin(position_ids * frequencies)\n",
    "        positional_encoding_table[:, 1::2] = torch.cos(position_ids * frequencies)\n",
    "    \n",
    "        # Registering the position enconding in state_dict but the its not included \n",
    "        # in named parameter as it is not trainable\n",
    "        self.register_buffer(\"positional_encoding_table\", positional_encoding_table)\n",
    "    \n",
    "    def forward(self, embeddings_batch):\n",
    "        \"\"\"\n",
    "        embeddings_batch shape = (batch size, seq_length, dmodel)\n",
    "        positional_encoding_table shape = (max_seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        assert embeddings_batch.ndim == 3, \\\n",
    "        f\"Embeddings batch should have dimension of 3 but got {embeddings_batch.ndim}\"\n",
    "        assert embeddings_batch.size()[-1] == self.positional_encoding_table.size()[-1], \\\n",
    "        f\"Embedding batch shape and positional_encoding_table shape should match, expected Embedding batch shape : {embeddings_batch.shape[-1]} while positional_encoding_table shape : {positional_encoding_table[-1]}\"\n",
    "        \n",
    "        # Get encodings for the given input sequence length\n",
    "        pos_encodings = self.positional_encoding_table[:embeddings_batch.shape[1]] # Choose only seq_length out of max_seq_length\n",
    "        \n",
    "        # Final output \n",
    "        out = embeddings_batch + pos_encodings\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f65ba77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.185497Z",
     "iopub.status.busy": "2024-01-20T21:46:32.184114Z",
     "iopub.status.idle": "2024-01-20T21:46:32.194091Z",
     "shell.execute_reply": "2024-01-20T21:46:32.192536Z"
    },
    "papermill": {
     "duration": 0.038215,
     "end_time": "2024-01-20T21:46:32.197151",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.158936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff, pdropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "        self.W1 = nn.Linear(dmodel, dff)      # Intermediate layer\n",
    "        self.W2 = nn.Linear(dff, dmodel)    # Output layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform Feedforward calculation\n",
    "        \n",
    "        x shape = (B - batch size, S/T - max token sequence length, D- model dimension).\n",
    "        \"\"\"\n",
    "        out = self.W2(self.relu(self.dropout(self.W1(x))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a15082c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.249100Z",
     "iopub.status.busy": "2024-01-20T21:46:32.247486Z",
     "iopub.status.idle": "2024-01-20T21:46:32.266877Z",
     "shell.execute_reply": "2024-01-20T21:46:32.265215Z"
    },
    "papermill": {
     "duration": 0.048055,
     "end_time": "2024-01-20T21:46:32.270048",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.221993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    We can refer to the following blog to understand in depth about the transformer and MHA\n",
    "    https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a\n",
    "    \n",
    "    Here we are clubbing all the linear layers together and duplicating the inputs and \n",
    "    then performing matrix multiplications\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h):\n",
    "        \"\"\"\n",
    "        Input Args:\n",
    "        \n",
    "        dk(int): Key dimensions used for generating Key weight matrix\n",
    "        dv(int): Val dimensions used for generating val weight matrix\n",
    "        h(int) : Number of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dk == dv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.h = h\n",
    "        self.dmodel = self.dk * self.h  # model dimension\n",
    "        \n",
    "        # Add the params in modulelist as the params in the conv list needs to be tracked\n",
    "        # wq, wk, wv -> multiple linear weights for the number of heads\n",
    "        self.WQ = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WK = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        self.WV = nn.Linear(self.dmodel, self.dmodel) # shape -> (dmodel, dmodel)\n",
    "        # Output Weights\n",
    "        self.WO = nn.Linear(self.h*self.dv, self.dmodel)  # shape -> (dmodel, dmodel)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, query, key, val, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for MHA\n",
    "        \n",
    "        X has a size of (batch_size, seq_length, d_model)\n",
    "        Wq, Wk, and Wv have a size of (d_model, d_model)\n",
    "        \n",
    "        Perform Scaled Dot Product Attention on multi head attention. \n",
    "        \n",
    "        Notation: B - batch size, S/T - max src/trg token-sequence length\n",
    "        query shape = (B, S, dmodel)\n",
    "        key shape = (B, S, dmodel)\n",
    "        val shape = (B, S, dmodel)\n",
    "        \"\"\"      \n",
    "        \n",
    "        # Weight the queries\n",
    "        Q = self.WQ(query)     # shape -> (B, S, dmodel)\n",
    "        K = self.WK(key)       # shape -> (B, S, dmodel)\n",
    "        V = self.WV(val)       # shape -> (B, S, dmodel)\n",
    "        \n",
    "        # Separate last dimension to number of head and dk\n",
    "        batch_size = Q.size(0)   \n",
    "        Q = Q.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        K = K.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        V = V.view(batch_size, -1, self.h, self.dk)   # shape -> (B, S, h, dk)\n",
    "        \n",
    "        # each sequence is split across n_heads, with each head receiving seq_length tokens \n",
    "        # with d_key elements in each token instead of d_model.\n",
    "        Q = Q.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        K = K.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        V = V.permute(0, 2, 1, 3) # shape -> (B, h, S, dk)\n",
    "        \n",
    "        # dot product of Q and K\n",
    "        scaled_dot_product = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.dk)\n",
    "        \n",
    "        # fill those positions of product as (-1e10) where mask positions are 0\n",
    "        if mask is not None:\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        attention_prob = scaled_dot_product\n",
    "        \n",
    "        # Create head \n",
    "        head = torch.matmul(scaled_dot_product, V)  # shape -> (B, h, S, S) * (B, h, S, dk) = (B, h, S, dk)\n",
    "        # Prepare the head to pass it through output linear layer\n",
    "        head = head.permute(0, 2, 1, 3).contiguous()  # shape -> (B, S, h, dk)\n",
    "        # Concatenate the head together\n",
    "        head = head.view(batch_size, -1, self.h* self.dk)  # shape -> (B, S, (h*dk = dmodel))\n",
    "        # Pass through output layer\n",
    "        token_representation = self.WO(head)\n",
    "        return token_representation, attention_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b99c7",
   "metadata": {
    "papermill": {
     "duration": 0.022617,
     "end_time": "2024-01-20T21:46:32.315495",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.292878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Encoder for the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aa44178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.365968Z",
     "iopub.status.busy": "2024-01-20T21:46:32.365168Z",
     "iopub.status.idle": "2024-01-20T21:46:32.375875Z",
     "shell.execute_reply": "2024-01-20T21:46:32.374649Z"
    },
    "papermill": {
     "duration": 0.039809,
     "end_time": "2024-01-20T21:46:32.379106",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.339297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This building block in the encoder layer consists of the following\n",
    "    1. MultiHead Attention\n",
    "    2. Sublayer Logic\n",
    "    3. Positional FeedForward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, h, dim_multiplier = 4, pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(dk, dv, h)\n",
    "        # Reference page 5 chapter 3.2.2 Multi-head attention\n",
    "        dmodel = dk*h\n",
    "        # Reference page 5 chapter 3.3 positionwise FeedForward\n",
    "        dff = dmodel * dim_multiplier\n",
    "        self.attn_norm = nn.LayerNorm(dmodel)\n",
    "        self.ff = PositionwiseFeedForward(dmodel, dff, pdropout=pdropout)\n",
    "        self.ff_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "    def forward(self, src_inputs, src_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass as per page 3 chapter 3.1\n",
    "        \"\"\"\n",
    "        mha_out, attention_wts = self.attention(\n",
    "                                query = src_inputs, \n",
    "                                key = src_inputs, \n",
    "                                val = src_inputs, \n",
    "                                mask = src_mask)\n",
    "        \n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        # Actual paper design is the following\n",
    "        intermediate_out = self.attn_norm(src_inputs + self.dropout(mha_out))\n",
    "        \n",
    "        pff_out = self.ff(intermediate_out)\n",
    "        \n",
    "        # Perform Add Norm again\n",
    "        out = self.ff_norm(intermediate_out + self.dropout(pff_out))\n",
    "        return out, attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ec690dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.427779Z",
     "iopub.status.busy": "2024-01-20T21:46:32.427228Z",
     "iopub.status.idle": "2024-01-20T21:46:32.436837Z",
     "shell.execute_reply": "2024-01-20T21:46:32.435484Z"
    },
    "papermill": {
     "duration": 0.037811,
     "end_time": "2024-01-20T21:46:32.440169",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.402358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dk, dv, h, num_encoders, dim_multiplier = 4, pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(dk, \n",
    "                         dv, \n",
    "                         h, \n",
    "                         dim_multiplier, \n",
    "                         pdropout) for _ in range(num_encoders)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, src_inputs, src_mask = None):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        src_inputs = (B - batch size, S/T - max token sequence length, D- model dimension)\n",
    "        \"\"\"\n",
    "        src_representation = src_inputs\n",
    "        \n",
    "        # Forward pass through encoder stack\n",
    "        for enc in self.encoder_layers:\n",
    "            src_representation, attention_wts = enc(src_representation, src_mask)\n",
    "            \n",
    "        self.attention_wts = attention_wts\n",
    "        return src_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02f310",
   "metadata": {
    "papermill": {
     "duration": 0.025398,
     "end_time": "2024-01-20T21:46:32.490199",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.464801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22d5b4b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.546311Z",
     "iopub.status.busy": "2024-01-20T21:46:32.545339Z",
     "iopub.status.idle": "2024-01-20T21:46:32.560377Z",
     "shell.execute_reply": "2024-01-20T21:46:32.559278Z"
    },
    "papermill": {
     "duration": 0.045648,
     "end_time": "2024-01-20T21:46:32.563340",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.517692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "                self, \n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                dim_multiplier = 4, \n",
    "                pdropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reference page 5 chapter 3.2.2 Multi-head attention\n",
    "        dmodel = dk*h\n",
    "        # Reference page 5 chapter 3.3 positionwise FeedForward\n",
    "        dff = dmodel * dim_multiplier\n",
    "        \n",
    "        # Masked Multi Head Attention\n",
    "        self.masked_attention = MultiHeadAttention(dk, dv, h)\n",
    "        self.masked_attn_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        # Multi head attention\n",
    "        self.attention = MultiHeadAttention(dk, dv, h)\n",
    "        self.attn_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        # Add position FeedForward Network\n",
    "        self.ff = PositionwiseFeedForward(dmodel, dff, pdropout=pdropout)\n",
    "        self.ff_norm = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = pdropout)\n",
    "        \n",
    "    def forward(self, target_inputs, src_inputs, target_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        target_inputs = embedded sequences    (batch_size, trg_seq_length, d_model)\n",
    "        src_inputs = embedded sequences       (batch_size, src_seq_length, d_model)\n",
    "        target_mask = mask for the sequences  (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        src_mask = mask for the sequences     (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        \n",
    "        mmha_out, attention_wts = self.masked_attention(\n",
    "                                query = target_inputs, \n",
    "                                key = target_inputs, \n",
    "                                val = target_inputs, \n",
    "                                mask = target_mask)\n",
    "        \n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        # Actual paper design is the following\n",
    "        target_inputs = self.masked_attn_norm(target_inputs + self.dropout(mmha_out))\n",
    "        \n",
    "        # Inputs to the decoder attention is given as follows\n",
    "        # query = previous decoder layer\n",
    "        # key and val = output of encoder\n",
    "        # mask = src_mask\n",
    "        # Reference : page 5 chapter 3.2.3 point 1\n",
    "        mha_out, attention_wts = self.attention(\n",
    "                                query = target_inputs, \n",
    "                                key = src_inputs, \n",
    "                                val = src_inputs, \n",
    "                                mask = src_mask)\n",
    "        target_inputs = self.attn_norm(target_inputs + self.dropout(mha_out))\n",
    "        \n",
    "        pff_out = self.ff(target_inputs)\n",
    "        # Perform Add Norm again\n",
    "        out = self.ff_norm(target_inputs + self.dropout(pff_out))\n",
    "        return out, attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "903d9c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.615214Z",
     "iopub.status.busy": "2024-01-20T21:46:32.614703Z",
     "iopub.status.idle": "2024-01-20T21:46:32.625200Z",
     "shell.execute_reply": "2024-01-20T21:46:32.623742Z"
    },
    "papermill": {
     "duration": 0.039954,
     "end_time": "2024-01-20T21:46:32.627940",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.587986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "                self, \n",
    "                dk, \n",
    "                dv, \n",
    "                h, \n",
    "                num_decoders, \n",
    "                dim_multiplier = 4, \n",
    "                pdropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(dk, \n",
    "                         dv, \n",
    "                         h, \n",
    "                         dim_multiplier, \n",
    "                         pdropout) for _ in range(num_decoders)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, target_inputs, src_inputs, target_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input from the Embedding layer\n",
    "        target_inputs = embedded sequences    (batch_size, trg_seq_length, d_model)\n",
    "        src_inputs = embedded sequences       (batch_size, src_seq_length, d_model)\n",
    "        target_mask = mask for the sequences  (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        src_mask = mask for the sequences     (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        target_representation = target_inputs\n",
    "        \n",
    "        # Forward pass through decoder stack\n",
    "        for layer in self.decoder_layers:\n",
    "            target_representation = layer(\n",
    "                                    target_representation,\n",
    "                                    src_inputs, \n",
    "                                    target_mask,\n",
    "                                    src_mask)\n",
    "        return target_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d100f",
   "metadata": {
    "papermill": {
     "duration": 0.023844,
     "end_time": "2024-01-20T21:46:32.678299",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.654455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Adding all up to construct the complete model for language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52789e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.730367Z",
     "iopub.status.busy": "2024-01-20T21:46:32.729938Z",
     "iopub.status.idle": "2024-01-20T21:46:32.753812Z",
     "shell.execute_reply": "2024-01-20T21:46:32.752470Z"
    },
    "papermill": {
     "duration": 0.053931,
     "end_time": "2024-01-20T21:46:32.757106",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.703175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                src_vocab_size,\n",
    "                target_vocab_size,\n",
    "                num_encoders,\n",
    "                num_decoders,\n",
    "                dim_multiplier = 4, \n",
    "                pdropout=0.1,\n",
    "                device = \"cpu\"\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reference page 5 chapter 3.2.2 Multi-head attention\n",
    "        dmodel = dk*h\n",
    "        # Modules required to build Encoder\n",
    "        self.src_embeddings = Embedding(src_vocab_size, dmodel)\n",
    "        self.src_positional_encoding = PositionalEncoding(\n",
    "                                        dmodel,\n",
    "                                        max_seq_length = src_vocab_size,\n",
    "                                        pdropout = pdropout\n",
    "                                        )\n",
    "        self.encoder = Encoder(\n",
    "                                dk, \n",
    "                                dv, \n",
    "                                h, \n",
    "                                num_encoders, \n",
    "                                dim_multiplier=dim_multiplier, \n",
    "                                pdropout=pdropout)\n",
    "        \n",
    "        # Modules required to build Decoder\n",
    "        self.target_embeddings = Embedding(target_vocab_size, dmodel)\n",
    "        self.target_positional_encoding = PositionalEncoding(\n",
    "                                        dmodel,\n",
    "                                        max_seq_length = target_vocab_size,\n",
    "                                        pdropout = pdropout\n",
    "                                        )\n",
    "        self.decoder = Decoder(\n",
    "                                dk, \n",
    "                                dv, \n",
    "                                h, \n",
    "                                num_decoders,  \n",
    "                                dim_multiplier=4, \n",
    "                                pdropout=0.1)\n",
    "        \n",
    "        # Final output \n",
    "        self.linear = nn.Linear(dmodel, target_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "        self.init_params()  \n",
    "        \n",
    "    # This part wasn't mentioned in the paper, but it's super important!\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        xavier has tremendous impact! I didn't expect\n",
    "        that the model's perf, with normalization layers, \n",
    "        is so dependent on the choice of weight initialization.\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def make_src_mask(self, src, src_pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: raw sequences with padding        (batch_size, seq_length) \n",
    "            src_pad_idx(int): index where the token need not be attended\n",
    "\n",
    "        Returns:\n",
    "            src_mask: mask for each sequence            (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        # assign 1 to tokens that need attended to and 0 to padding tokens, \n",
    "        # then add 2 dimensions\n",
    "        src_mask = (src != src_pad_idx).view(batch_size, 1, 1, -1)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_target_mask(self, target, target_pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target:  raw sequences with padding        (batch_size, seq_length)     \n",
    "            target_pad_idx(int): index where the token need not be attended\n",
    "\n",
    "        Returns:\n",
    "            target_mask: mask for each sequence   (batch_size, 1, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_length = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        # assign True to tokens that need attended to and \n",
    "        # False to padding tokens, then add 2 dimensions\n",
    "        target_mask = (trg != target_pad_idx).view(batch_size, 1, 1, -1) # (batch_size, 1, 1, seq_length)\n",
    "\n",
    "        # generate subsequent mask\n",
    "        trg_sub_mask = torch.tril(torch.ones((seq_length, seq_length), device=self.device)).bool() # (batch_size, 1, seq_length, seq_length)\n",
    "\n",
    "        # bitwise \"and\" operator | 0 & 0 = 0, 1 & 1 = 1, 1 & 0 = 0\n",
    "        target_mask = target_mask & trg_sub_mask\n",
    "\n",
    "        return target_mask\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        src_token_ids_batch, \n",
    "        target_token_ids_batch, \n",
    "        src_pad_idx, \n",
    "        target_pad_idx):\n",
    "        \n",
    "        # create source and target masks     \n",
    "        src_mask = self.make_src_mask(\n",
    "                        src_token_ids_batch, \n",
    "                        src_pad_idx) # (batch_size, 1, 1, src_seq_length)\n",
    "        target_mask = self.make_target_mask(\n",
    "                        target_token_ids_batch, \n",
    "                        target_pad_idx) # (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "        \n",
    "        # Create embeddings\n",
    "        src_representations = self.src_embeddings(src_token_ids_batch)\n",
    "        src_representations = self.src_positional_encoding(src_representations)\n",
    "        \n",
    "        target_representations = self.target_embeddings(target_token_ids_batch)\n",
    "        target_representations = self.target_positional_encoding(target_representations)\n",
    "        \n",
    "        # Encode \n",
    "        encoded_src = self.encoder(src_representations, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_output = self.decoder(\n",
    "                                target_representations, \n",
    "                                encoded_src, \n",
    "                                target_mask, \n",
    "                                src_mask)\n",
    "        \n",
    "        # Post processing\n",
    "        out = self.linear(decoded_output)\n",
    "        # Output \n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c13ac",
   "metadata": {
    "papermill": {
     "duration": 0.022788,
     "end_time": "2024-01-20T21:46:32.803889",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.781101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6715db47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:32.853551Z",
     "iopub.status.busy": "2024-01-20T21:46:32.853057Z",
     "iopub.status.idle": "2024-01-20T21:46:33.244697Z",
     "shell.execute_reply": "2024-01-20T21:46:33.242860Z"
    },
    "papermill": {
     "duration": 0.420596,
     "end_time": "2024-01-20T21:46:33.248190",
     "exception": false,
     "start_time": "2024-01-20T21:46:32.827594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embeddings): Embedding(\n",
      "    (embedding_lookup): Embedding(7983, 256)\n",
      "  )\n",
      "  (src_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-2): 3 x EncoderLayer(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WK): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WV): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WO): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (W2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (ff_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (target_embeddings): Embedding(\n",
      "    (embedding_lookup): Embedding(5979, 256)\n",
      "  )\n",
      "  (target_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-2): 3 x DecoderLayer(\n",
      "        (masked_attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WK): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WV): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WO): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (masked_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): MultiHeadAttention(\n",
      "          (WQ): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WK): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WV): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (WO): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (W2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (ff_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=256, out_features=5979, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dk = 32\n",
    "dv = 32\n",
    "h = 8\n",
    "src_vocab_size = len(vocab_src)\n",
    "target_vocab_size = len(vocab_trg)\n",
    "num_encoders = 3\n",
    "num_decoders = 3\n",
    "dim_multiplier = 4\n",
    "pdropout=0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(\n",
    "                dk, \n",
    "                dv, \n",
    "                h,\n",
    "                src_vocab_size,\n",
    "                target_vocab_size,\n",
    "                num_encoders,\n",
    "                num_decoders,\n",
    "                dim_multiplier, \n",
    "                pdropout,\n",
    "                device = device)\n",
    "# model.cuda()\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8472e3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:33.298233Z",
     "iopub.status.busy": "2024-01-20T21:46:33.297532Z",
     "iopub.status.idle": "2024-01-20T21:46:33.307106Z",
     "shell.execute_reply": "2024-01-20T21:46:33.305598Z"
    },
    "papermill": {
     "duration": 0.038682,
     "end_time": "2024-01-20T21:46:33.310396",
     "exception": false,
     "start_time": "2024-01-20T21:46:33.271714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,640,475 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928e724",
   "metadata": {
    "papermill": {
     "duration": 0.024021,
     "end_time": "2024-01-20T21:46:33.358494",
     "exception": false,
     "start_time": "2024-01-20T21:46:33.334473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c07c1e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T21:46:33.408699Z",
     "iopub.status.busy": "2024-01-20T21:46:33.408159Z",
     "iopub.status.idle": "2024-01-20T21:46:33.418262Z",
     "shell.execute_reply": "2024-01-20T21:46:33.416616Z"
    },
    "papermill": {
     "duration": 0.038598,
     "end_time": "2024-01-20T21:46:33.421003",
     "exception": false,
     "start_time": "2024-01-20T21:46:33.382405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1800581,
     "sourceId": 2936819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.44305,
   "end_time": "2024-01-20T21:46:35.575982",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-20T21:45:26.132932",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
